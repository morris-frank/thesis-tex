
@article{ahissarReverse2009,
  title = {Reverse Hierarchies and Sensory Learning},
  author = {Ahissar, Merav and Nahum, Mor and Nelken, Israel and Hochstein, Shaul},
  date = {2009-02-12},
  journaltitle = {Philos Trans R Soc Lond B Biol Sci},
  volume = {364},
  pages = {285--299},
  issn = {0962-8436},
  doi = {10.1098/rstb.2008.0253},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2674477/},
  urldate = {2019-11-01},
  abstract = {Revealing the relationships between perceptual representations in the brain and mechanisms of adult perceptual learning is of great importance, potentially leading to significantly improved training techniques both for improving skills in the general population and for ameliorating deficits in special populations. In this review, we summarize the essentials of reverse hierarchy theory for perceptual learning in the visual and auditory modalities and describe the theory's implications for designing improved training procedures, for a variety of goals and populations.},
  eprint = {18986968},
  eprinttype = {pmid},
  file = {/home/morris/docs/lib/Zotero/storage/URVTGP5I/Ahissar et al. - 2009 - Reverse hierarchies and sensory learning.pdf},
  keywords = {read},
  number = {1515},
  pmcid = {PMC2674477}
}

@article{alemiFixing2018,
  title = {Fixing a {{Broken ELBO}}},
  author = {Alemi, Alexander A. and Poole, Ben and Fischer, Ian and Dillon, Joshua V. and Saurous, Rif A. and Murphy, Kevin},
  date = {2018-02-13},
  url = {http://arxiv.org/abs/1711.00464},
  urldate = {2019-11-21},
  abstract = {Recent work in unsupervised representation learning has focused on learning deep directed latent-variable models. Fitting these models by maximizing the marginal likelihood or evidence is typically intractable, thus a common approximation is to maximize the evidence lower bound (ELBO) instead. However, maximum likelihood training (whether exact or approximate) does not necessarily result in a good latent representation, as we demonstrate both theoretically and empirically. In particular, we derive variational lower and upper bounds on the mutual information between the input and the latent variable, and use these bounds to derive a rate-distortion curve that characterizes the tradeoff between compression and reconstruction accuracy. Using this framework, we demonstrate that there is a family of models with identical ELBO, but different quantitative and qualitative characteristics. Our framework also suggests a simple new method to ensure that latent variable models with powerful stochastic decoders do not ignore their latent code.},
  archivePrefix = {arXiv},
  eprint = {1711.00464},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/X2YG5EN6/Alemi et al. - 2018 - Fixing a Broken ELBO.pdf;/home/morris/docs/lib/Zotero/storage/NH9N82FN/1711.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@unpublished{andreasjanssonSinging2017,
  title = {Singing {{Voice Separation}} with {{Deep U}}-{{Net Convolutional Networks}}.},
  author = {{Andreas Jansson} and {Eric J. Humphrey} and {Nicola Montecchio} and {Rachel M. Bittner} and {Aparna Kumar} and {Tillman Weyde}},
  date = {2017-10-23},
  url = {https://zenodo.org/record/1414934},
  urldate = {2019-10-11},
  abstract = {[TODO] Add abstract here.},
  file = {/home/morris/docs/lib/Zotero/storage/7AFHLCNK/Andreas Jansson et al. - 2017 - Singing Voice Separation with Deep U-Net Convoluti.pdf},
  keywords = {read},
  venue = {{Suzhou, China}}
}

@article{atanovSemiConditional2019,
  title = {Semi-{{Conditional Normalizing Flows}} for {{Semi}}-{{Supervised Learning}}},
  author = {Atanov, Andrei and Volokhova, Alexandra and Ashukha, Arsenii and Sosnovik, Ivan and Vetrov, Dmitry},
  date = {2019-05-01},
  url = {http://arxiv.org/abs/1905.00505},
  urldate = {2020-01-22},
  abstract = {This paper proposes a semi-conditional normalizing flow model for semi-supervised learning. The model uses both labelled and unlabeled data to learn an explicit model of joint distribution over objects and labels. Semi-conditional architecture of the model allows us to efficiently compute a value and gradients of the marginal likelihood for unlabeled objects. The conditional part of the model is based on a proposed conditional coupling layer. We demonstrate performance of the model for semi-supervised classification problem on different datasets. The model outperforms the baseline approach based on variational auto-encoders on MNIST dataset.},
  archivePrefix = {arXiv},
  eprint = {1905.00505},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/29RRZS58/Atanov et al. - 2019 - Semi-Conditional Normalizing Flows for Semi-Superv.pdf;/home/morris/docs/lib/Zotero/storage/BHUNYDUH/1905.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{baarsFundamentals2018,
  title = {Fundamentals of Cognitive Neuroscience: A Beginner's Guide},
  shorttitle = {Fundamentals of Cognitive Neuroscience},
  author = {Baars, Bernard and Gage, Nicole M.},
  date = {2018},
  edition = {2},
  publisher = {{Academic Press}},
  file = {/home/morris/docs/lib/Zotero/storage/WQZ63APH/Baars and Gage - 2018 - Fundamentals of cognitive neuroscience a beginner.pdf;/home/morris/docs/lib/Zotero/storage/I674JT88/books.html},
  isbn = {978-0-12-803813-0},
  keywords = {out}
}

@article{baiEmpirical2018,
  title = {An {{Empirical Evaluation}} of {{Generic Convolutional}} and {{Recurrent Networks}} for {{Sequence Modeling}}},
  author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
  date = {2018-03-03},
  url = {http://arxiv.org/abs/1803.01271},
  urldate = {2019-10-17},
  abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
  archivePrefix = {arXiv},
  eprint = {1803.01271},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/W2QPMHIC/Bai et al. - 2018 - An Empirical Evaluation of Generic Convolutional a.pdf;/home/morris/docs/lib/Zotero/storage/82UEK6S4/1803.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,look},
  primaryClass = {cs}
}

@article{battagliaRelational2018,
  title = {Relational Inductive Biases, Deep Learning, and Graph Networks},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  date = {2018-06-04},
  url = {http://arxiv.org/abs/1806.01261},
  urldate = {2019-10-09},
  abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  archivePrefix = {arXiv},
  eprint = {1806.01261},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/2YNW3G7R/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf;/home/morris/docs/lib/Zotero/storage/WVB25X38/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf;/home/morris/docs/lib/Zotero/storage/PZNKSCFI/1806.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{bizleyAuditory2013,
  title = {Auditory {{Cortex Represents Both Pitch Judgments}} and the {{Corresponding Acoustic Cues}}},
  author = {Bizley, Jennifer K. and Walker, Kerry M.M. and Nodal, Fernando R. and King, Andrew J. and Schnupp, Jan W.H.},
  date = {2013-04-08},
  journaltitle = {Curr Biol},
  volume = {23},
  pages = {620--625},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2013.03.003},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3696731/},
  urldate = {2019-10-31},
  abstract = {The neural processing of sensory stimuli involves a transformation of physical stimulus parameters into perceptual features, and elucidating where and how this transformation occurs is one of the ultimate aims of sensory neurophysiology. Recent studies have shown that the firing of neurons in early sensory cortex can be modulated by multisensory interactions [], motor behavior [], and reward feedback [], but it remains unclear whether neural activity is more closely tied to perception, as indicated by behavioral choice, or to the physical properties of the stimulus. We investigated which of these properties are predominantly represented in auditory cortex by recording local field potentials (LFPs) and multiunit spiking activity in ferrets while they discriminated the pitch of artificial vowels. We found that auditory cortical activity is informative both about the fundamental frequency (F0) of a target sound and also about the pitch that the animals appear to perceive given their behavioral responses. Surprisingly, although the stimulus F0 was well represented at the onset of the target sound, neural activity throughout auditory cortex frequently predicted the reported pitch better than the target F0., ► Auditory cortical responses were recorded while ferrets discriminated pitch shifts ► LFP and multiunit activity are sensitive to the sound’s fundamental frequency (F0) ► Neural activity related to animals’ reported pitch increases throughout the trial ► Cortical responses were more informative about behavioral choices than the sound F0},
  eprint = {23523247},
  eprinttype = {pmid},
  file = {/home/morris/docs/lib/Zotero/storage/S2NWXQVN/Bizley et al. - 2013 - Auditory Cortex Represents Both Pitch Judgments an.pdf;/home/morris/docs/lib/Zotero/storage/YSFYRUAS/bizley2013.pdf},
  keywords = {out},
  number = {7},
  pmcid = {PMC3696731}
}

@article{bizleyWhat2013,
  title = {The What, Where and How of Auditory-Object Perception},
  author = {Bizley, Jennifer K. and Cohen, Yale E.},
  date = {2013-10},
  journaltitle = {Nat. Rev. Neurosci.},
  volume = {14},
  pages = {693--707},
  issn = {1471-0048},
  doi = {10.1038/nrn3565},
  abstract = {The fundamental perceptual unit in hearing is the 'auditory object'. Similar to visual objects, auditory objects are the computational result of the auditory system's capacity to detect, extract, segregate and group spectrotemporal regularities in the acoustic environment; the multitude of acoustic stimuli around us together form the auditory scene. However, unlike the visual scene, resolving the component objects within the auditory scene crucially depends on their temporal structure. Neural correlates of auditory objects are found throughout the auditory system. However, neural responses do not become correlated with a listener's perceptual reports until the level of the cortex. The roles of different neural structures and the contribution of different cognitive states to the perception of auditory objects are not yet fully understood.},
  eprint = {24052177},
  eprinttype = {pmid},
  file = {/home/morris/docs/lib/Zotero/storage/MU2W33N2/bizley2013.pdf},
  keywords = {Acoustic Stimulation,Acoustics,Animals,Auditory Cortex,Auditory Pathways,Auditory Perception,Humans,read},
  langid = {english},
  number = {10},
  pmcid = {PMC4082027}
}

@article{burgessMONet2019,
  title = {{{MONet}}: {{Unsupervised Scene Decomposition}} and {{Representation}}},
  shorttitle = {{{MONet}}},
  author = {Burgess, Christopher P. and Matthey, Loic and Watters, Nicholas and Kabra, Rishabh and Higgins, Irina and Botvinick, Matt and Lerchner, Alexander},
  date = {2019-01-22},
  url = {http://arxiv.org/abs/1901.11390},
  urldate = {2019-10-07},
  abstract = {The ability to decompose scenes in terms of abstract building blocks is crucial for general intelligence. Where those basic building blocks share meaningful properties, interactions and other regularities across scenes, such decompositions can simplify reasoning and facilitate imagination of novel scenarios. In particular, representing perceptual observations in terms of entities should improve data efficiency and transfer performance on a wide range of tasks. Thus we need models capable of discovering useful decompositions of scenes by identifying units with such regularities and representing them in a common format. To address this problem, we have developed the Multi-Object Network (MONet). In this model, a VAE is trained end-to-end together with a recurrent attention network -- in a purely unsupervised manner -- to provide attention masks around, and reconstructions of, regions of images. We show that this model is capable of learning to decompose and represent challenging 3D scenes into semantically meaningful components, such as objects and background elements.},
  archivePrefix = {arXiv},
  eprint = {1901.11390},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/7IYWKLKI/Burgess et al. - 2019 - MONet Unsupervised Scene Decomposition and Repres.pdf;/home/morris/docs/lib/Zotero/storage/DNNPQY3L/1901.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{burgessUnderstanding2018,
  title = {Understanding Disentangling in {{Beta}}-{{VAE}}},
  author = {Burgess, Christopher P. and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
  date = {2018-04-10},
  url = {http://arxiv.org/abs/1804.03599},
  urldate = {2019-10-17},
  abstract = {We present new intuitions and theoretical assessments of the emergence of disentangled representation in variational autoencoders. Taking a rate-distortion theory perspective, we show the circumstances under which representations aligned with the underlying generative factors of variation of data emerge when optimising the modified ELBO bound in \$\textbackslash beta\$-VAE, as training progresses. From these insights, we propose a modification to the training regime of \$\textbackslash beta\$-VAE, that progressively increases the information capacity of the latent code during training. This modification facilitates the robust learning of disentangled representations in \$\textbackslash beta\$-VAE, without the previous trade-off in reconstruction accuracy.},
  archivePrefix = {arXiv},
  eprint = {1804.03599},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/NP7GIL4L/Burgess et al. - 2018 - Understanding disentangling in $beta$-VAE.pdf;/home/morris/docs/lib/Zotero/storage/VE3M6RPQ/1804.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,look,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{chandnaMonoaural2017,
  title = {Monoaural {{Audio Source Separation Using Deep Convolutional Neural Networks}}},
  booktitle = {Latent {{Variable Analysis}} and {{Signal Separation}}},
  author = {Chandna, Pritish and Miron, Marius and Janer, Jordi and Gómez, Emilia},
  editor = {Tichavský, Petr and Babaie-Zadeh, Massoud and Michel, Olivier J.J. and Thirion-Moreau, Nadège},
  date = {2017},
  pages = {258--266},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-53547-0_25},
  abstract = {In this paper we introduce a low-latency monaural source separation framework using a Convolutional Neural Network (CNN). We use a CNN to estimate time-frequency soft masks which are applied for source separation. We evaluate the performance of the neural network on a database comprising of musical mixtures of three instruments: voice, drums, bass as well as other instruments which vary from song to song. The proposed architecture is compared to a Multilayer Perceptron (MLP), achieving on-par results and a significant improvement in processing time. The algorithm was submitted to source separation evaluation campaigns to test efficiency, and achieved competitive results.},
  file = {/home/morris/docs/lib/Zotero/storage/NA77MJUB/Chandna et al. - 2017 - Monoaural Audio Source Separation Using Deep Convo.pdf},
  isbn = {978-3-319-53547-0},
  keywords = {Convolutional autoencoder,Convolutional Neural Networks,Deep learning,Low-latency,Music source separation},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{chenRobust2019,
  title = {Robust {{Ordinal VAE}}: {{Employing Noisy Pairwise Comparisons}} for {{Disentanglement}}},
  shorttitle = {Robust {{Ordinal VAE}}},
  author = {Chen, Junxiang and Batmanghelich, Kayhan},
  date = {2019-10-13},
  url = {http://arxiv.org/abs/1910.05898},
  urldate = {2019-10-29},
  abstract = {Recent work by Locatello et al. (2018) has shown that an inductive bias is required to disentangle factors of interest in Variational Autoencoder (VAE). Motivated by a real-world problem, we propose a setting where such bias is introduced by providing pairwise ordinal comparisons between instances, based on the desired factor to be disentangled. For example, a doctor compares pairs of patients based on the level of severity of their illnesses, and the desired factor is a quantitive level of the disease severity. In a real-world application, the pairwise comparisons are usually noisy. Our method, Robust Ordinal VAE (ROVAE), incorporates the noisy pairwise ordinal comparisons in the disentanglement task. We introduce non-negative random variables in ROVAE, such that it can automatically determine whether each pairwise ordinal comparison is trustworthy and ignore the noisy comparisons. Experimental results demonstrate that ROVAE outperforms existing methods and is more robust to noisy pairwise comparisons in both benchmark datasets and a real-world application.},
  archivePrefix = {arXiv},
  eprint = {1910.05898},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/6WEEP433/Chen and Batmanghelich - 2019 - Robust Ordinal VAE Employing Noisy Pairwise Compa.pdf;/home/morris/docs/lib/Zotero/storage/PU947WEA/1910.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{chenVariational2016,
  title = {Variational {{Lossy Autoencoder}}},
  author = {Chen, Xi and Kingma, Diederik P. and Salimans, Tim and Duan, Yan and Dhariwal, Prafulla and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  date = {2016-11-08},
  url = {http://arxiv.org/abs/1611.02731},
  urldate = {2019-10-17},
  abstract = {Representation learning seeks to expose certain aspects of observed data in a learned representation that's amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and , by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the VAE only "autoencodes" data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution \$p(z)\$ and decoding distribution \$p(x|z)\$, we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks.},
  archivePrefix = {arXiv},
  eprint = {1611.02731},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/JUYFWQY9/Chen et al. - 2016 - Variational Lossy Autoencoder.pdf;/home/morris/docs/lib/Zotero/storage/JRPEZQKU/1611.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{chorowskiUnsupervised2019,
  title = {Unsupervised Speech Representation Learning Using {{WaveNet}} Autoencoders},
  author = {Chorowski, Jan and Weiss, Ron J. and Bengio, Samy and van den Oord, Aäron},
  date = {2019-12},
  journaltitle = {IEEE/ACM Trans. Audio Speech Lang. Process.},
  volume = {27},
  pages = {2041--2053},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2019.2938863},
  url = {http://arxiv.org/abs/1901.08810},
  urldate = {2019-10-17},
  abstract = {We consider the task of unsupervised extraction of meaningful latent representations of speech by applying autoencoding neural networks to speech waveforms. The goal is to learn a representation able to capture high level semantic content from the signal, e.g.\textbackslash{} phoneme identities, while being invariant to confounding low level details in the signal such as the underlying pitch contour or background noise. Since the learned representation is tuned to contain only phonetic content, we resort to using a high capacity WaveNet decoder to infer information discarded by the encoder from previous samples. Moreover, the behavior of autoencoder models depends on the kind of constraint that is applied to the latent representation. We compare three variants: a simple dimensionality reduction bottleneck, a Gaussian Variational Autoencoder (VAE), and a discrete Vector Quantized VAE (VQ-VAE). We analyze the quality of learned representations in terms of speaker independence, the ability to predict phonetic content, and the ability to accurately reconstruct individual spectrogram frames. Moreover, for discrete encodings extracted using the VQ-VAE, we measure the ease of mapping them to phonemes. We introduce a regularization scheme that forces the representations to focus on the phonetic content of the utterance and report performance comparable with the top entries in the ZeroSpeech 2017 unsupervised acoustic unit discovery task.},
  archivePrefix = {arXiv},
  eprint = {1901.08810},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/89YT3HGD/Chorowski et al. - 2019 - Unsupervised speech representation learning using .pdf;/home/morris/docs/lib/Zotero/storage/L6RRF3S5/1901.html},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing,look,Statistics - Machine Learning},
  number = {12},
  options = {useprefix=true}
}

@article{cohen-hadriaImproving2019,
  title = {Improving Singing Voice Separation Using {{Deep U}}-{{Net}} and {{Wave}}-{{U}}-{{Net}} with Data Augmentation},
  author = {Cohen-Hadria, Alice and Roebel, Axel and Peeters, Geoffroy},
  date = {2019-03-04},
  url = {http://arxiv.org/abs/1903.01415},
  urldate = {2020-06-24},
  abstract = {State-of-the-art singing voice separation is based on deep learning making use of CNN structures with skip connections (like U-net model, Wave-U-Net model, or MSDENSELSTM). A key to the success of these models is the availability of a large amount of training data. In the following study, we are interested in singing voice separation for mono signals and will investigate into comparing the U-Net and the Wave-U-Net that are structurally similar, but work on different input representations. First, we report a few results on variations of the U-Net model. Second, we will discuss the potential of state of the art speech and music transformation algorithms for augmentation of existing data sets and demonstrate that the effect of these augmentations depends on the signal representations used by the model. The results demonstrate a considerable improvement due to the augmentation for both models. But pitch transposition is the most effective augmentation strategy for the U-Net model, while transposition, time stretching, and formant shifting have a much more balanced effect on the Wave-U-Net model. Finally, we compare the two models on the same dataset.},
  archivePrefix = {arXiv},
  eprint = {1903.01415},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/MI8DL8WW/Cohen-Hadria et al. - 2019 - Improving singing voice separation using Deep U-Ne.pdf},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{dauphinLanguage2017,
  title = {Language {{Modeling}} with {{Gated Convolutional Networks}}},
  author = {Dauphin, Yann N. and Fan, Angela and Auli, Michael and Grangier, David},
  date = {2017-09-08},
  url = {http://arxiv.org/abs/1612.08083},
  urldate = {2020-06-24},
  abstract = {The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.},
  archivePrefix = {arXiv},
  eprint = {1612.08083},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/XCIF4XM3/Dauphin et al. - 2017 - Language Modeling with Gated Convolutional Network.pdf},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{defossezDemucs2019,
  title = {Demucs: {{Deep Extractor}} for {{Music Sources}} with Extra Unlabeled Data Remixed},
  shorttitle = {Demucs},
  author = {Défossez, Alexandre and Usunier, Nicolas and Bottou, Léon and Bach, Francis},
  date = {2019-09-03},
  url = {http://arxiv.org/abs/1909.01174},
  urldate = {2019-11-29},
  abstract = {We study the problem of source separation for music using deep learning with four known sources: drums, bass, vocals and other accompaniments. State-of-the-art approaches predict soft masks over mixture spectrograms while methods working on the waveform are lagging behind as measured on the standard MusDB benchmark. Our contribution is two fold. (i) We introduce a simple convolutional and recurrent model that outperforms the state-of-the-art model on waveforms, that is, Wave-U-Net, by 1.6 points of SDR (signal to distortion ratio). (ii) We propose a new scheme to leverage unlabeled music. We train a first model to extract parts with at least one source silent in unlabeled tracks, for instance without bass. We remix this extract with a bass line taken from the supervised dataset to form a new weakly supervised training example. Combining our architecture and scheme, we show that waveform methods can play in the same ballpark as spectrogram ones.},
  archivePrefix = {arXiv},
  eprint = {1909.01174},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/ER6GWA6N/Défossez et al. - 2019 - Demucs Deep Extractor for Music Sources with extr.pdf;/home/morris/docs/lib/Zotero/storage/IKL3N9VI/1909.html},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@inproceedings{defossezSING2018,
  title = {{{SING}}: {{Symbol}}-to-{{Instrument Neural Generator}}},
  shorttitle = {{{SING}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Defossez, Alexandre and Zeghidour, Neil and Usunier, Nicolas and Bottou, Leon and Bach, Francis},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  date = {2018},
  pages = {9041--9051},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/8118-sing-symbol-to-instrument-neural-generator.pdf},
  urldate = {2020-03-13},
  file = {/home/morris/docs/lib/Zotero/storage/FP2BFA7N/Defossez et al. - 2018 - SING Symbol-to-Instrument Neural Generator.pdf;/home/morris/docs/lib/Zotero/storage/X6JUD9KF/8118-sing-symbol-to-instrument-neural-generator.html}
}

@incollection{dielemanChallenge2018,
  title = {The Challenge of Realistic Music Generation: Modelling Raw Audio at Scale},
  shorttitle = {The Challenge of Realistic Music Generation},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Dieleman, Sander and van den Oord, Aäron and Simonyan, Karen},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  date = {2018},
  pages = {7989--7999},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/8023-the-challenge-of-realistic-music-generation-modelling-raw-audio-at-scale.pdf},
  urldate = {2019-10-17},
  file = {/home/morris/docs/lib/Zotero/storage/LGCIBU6Q/Dieleman et al. - 2018 - The challenge of realistic music generation model.pdf;/home/morris/docs/lib/Zotero/storage/TPQCGRPA/8023-the-challenge-of-realistic-music-generation-modelling-raw-audio-at-scale.html},
  keywords = {look-again},
  options = {useprefix=true}
}

@inproceedings{dielemanEndtoend2014,
  title = {End-to-End Learning for Music Audio},
  booktitle = {2014 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Dieleman, Sander and Schrauwen, Benjamin},
  date = {2014},
  pages = {6964--6968},
  publisher = {{IEEE}},
  file = {/home/morris/docs/lib/Zotero/storage/JUFG3B8N/Dieleman and Schrauwen - 2014 - End-to-end learning for music audio.pdf}
}

@article{dinhDensity2017,
  title = {Density Estimation Using {{Real NVP}}},
  author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  date = {2017-02-27},
  url = {http://arxiv.org/abs/1605.08803},
  urldate = {2019-12-03},
  abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
  archivePrefix = {arXiv},
  eprint = {1605.08803},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/ENC2LDYB/Dinh et al. - 2017 - Density estimation using Real NVP.pdf;/home/morris/docs/lib/Zotero/storage/WPBQR5CJ/1605.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{dinhNICE2015,
  title = {{{NICE}}: {{Non}}-Linear {{Independent Components Estimation}}},
  shorttitle = {{{NICE}}},
  author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
  date = {2015-04-10},
  url = {http://arxiv.org/abs/1410.8516},
  urldate = {2020-03-08},
  abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
  archivePrefix = {arXiv},
  eprint = {1410.8516},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/UWULPUW6/Dinh et al. - 2015 - NICE Non-linear Independent Components Estimation.pdf;/home/morris/docs/lib/Zotero/storage/QN35BUCA/1410.html},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@inproceedings{dutilleuxImplementation1990,
  title = {An {{Implementation}} of the “Algorithme à Trous” to {{Compute}} the {{Wavelet Transform}}},
  booktitle = {Wavelets},
  author = {Dutilleux, P.},
  editor = {Combes, Jean-Michel and Grossmann, Alexander and Tchamitchian, Philippe},
  date = {1990},
  pages = {298--304},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-75988-8_29},
  abstract = {The computation of the wavelet transform involves the computation of the convolution product of the signal to be analysed by the analysing wavelet. It will be shown that the computation load grows with the scale factor of the analysis. We are interested in musical sounds lasting a few seconds. Using a straightforward algorithm leads to a prohibitive computation time, so we need a more effective computation procedure.},
  isbn = {978-3-642-75988-8},
  keywords = {Analyse Wavelet,Digital Signal Processor,Finite Impulse Response,Finite Impulse Response Filter,Side Lobe},
  langid = {english},
  series = {Inverse Problems and Theoretical Imaging}
}

@article{engelNeural2017,
  title = {Neural {{Audio Synthesis}} of {{Musical Notes}} with {{WaveNet Autoencoders}}},
  author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Eck, Douglas and Simonyan, Karen and Norouzi, Mohammad},
  date = {2017-04-05},
  url = {http://arxiv.org/abs/1704.01279},
  urldate = {2019-10-17},
  abstract = {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.},
  archivePrefix = {arXiv},
  eprint = {1704.01279},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/C793VA92/Engel et al. - 2017 - Neural Audio Synthesis of Musical Notes with WaveN.pdf;/home/morris/docs/lib/Zotero/storage/AQ3U6QW5/1704.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Sound,look},
  primaryClass = {cs}
}

@unpublished{ethanmanilowNorthwestern2018,
  title = {The {{Northwestern University Source Separation Library}}},
  author = {{Ethan Manilow} and {Prem Seetharaman} and {Bryan Pardo}},
  date = {2018-09-23},
  url = {https://zenodo.org/record/1492407},
  urldate = {2019-10-10},
  abstract = {Audio source separation is the process of isolating individual sonic elements from a mixture or auditory scene. We present the Northwestern University Source Separation Library, or nussl for short. nussl (pronounced 'nuzzle') is an open-source, object-oriented audio source separation library implemented in Python. nussl provides implementations for many existing source separation algorithms and a platform for creating the next generation of source separation algorithms. By nature of its design, nussl easily allows new algorithms to be benchmarked against existing algorithms on established data sets and facilitates development of new variations on algorithms. Here, we present the design methodologies in nussl, two experiments using it, and use nussl to showcase benchmarks for some algorithms contained within.},
  file = {/home/morris/docs/lib/Zotero/storage/CYYZAQAT/Ethan Manilow et al. - 2018 - The Northwestern University Source Separation Libr.pdf},
  venue = {{Paris, France}}
}

@article{gabbayStyle2019,
  title = {Style {{Generator Inversion}} for {{Image Enhancement}} and {{Animation}}},
  author = {Gabbay, Aviv and Hoshen, Yedid},
  date = {2019-06-05},
  url = {http://arxiv.org/abs/1906.11880},
  urldate = {2019-10-29},
  abstract = {One of the main motivations for training high quality image generative models is their potential use as tools for image manipulation. Recently, generative adversarial networks (GANs) have been able to generate images of remarkable quality. Unfortunately, adversarially-trained unconditional generator networks have not been successful as image priors. One of the main requirements for a network to act as a generative image prior, is being able to generate every possible image from the target distribution. Adversarial learning often experiences mode-collapse, which manifests in generators that cannot generate some modes of the target distribution. Another requirement often not satisfied is invertibility i.e. having an efficient way of finding a valid input latent code given a required output image. In this work, we show that differently from earlier GANs, the very recently proposed style-generators are quite easy to invert. We use this important observation to propose style generators as general purpose image priors. We show that style generators outperform other GANs as well as Deep Image Prior as priors for image enhancement tasks. The latent space spanned by style-generators satisfies linear identity-pose relations. The latent space linearity, combined with invertibility, allows us to animate still facial images without supervision. Extensive experiments are performed to support the main contributions of this paper.},
  archivePrefix = {arXiv},
  eprint = {1906.11880},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/SPCUSFXC/Gabbay and Hoshen - 2019 - Style Generator Inversion for Image Enhancement an.pdf;/home/morris/docs/lib/Zotero/storage/TPGMKWZV/1906.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{garbaceaLow2019,
  title = {Low {{Bit}}-{{Rate Speech Coding}} with {{VQ}}-{{VAE}} and a {{WaveNet Decoder}}},
  author = {Gârbacea, Cristina and van den Oord, Aäron and Li, Yazhe and Lim, Felicia S. C. and Luebs, Alejandro and Vinyals, Oriol and Walters, Thomas C.},
  date = {2019-05},
  journaltitle = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {735--739},
  doi = {10.1109/ICASSP.2019.8683277},
  url = {http://arxiv.org/abs/1910.06464},
  urldate = {2020-01-22},
  abstract = {In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.},
  archivePrefix = {arXiv},
  eprint = {1910.06464},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/6NC69KKU/Gârbacea et al. - 2019 - Low Bit-Rate Speech Coding with VQ-VAE and a WaveN.pdf;/home/morris/docs/lib/Zotero/storage/TFF6G887/garbacea2019.html;/home/morris/docs/lib/Zotero/storage/ULYT8MQN/1910.html},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  options = {useprefix=true}
}

@inproceedings{gershmanAmortized2014,
  title = {Amortized Inference in Probabilistic Reasoning},
  booktitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  author = {Gershman, Samuel and Goodman, Noah},
  date = {2014},
  volume = {36},
  file = {/home/morris/docs/lib/Zotero/storage/K8PP36PX/Gershman and Goodman - 2014 - Amortized inference in probabilistic reasoning.pdf},
  number = {36}
}

@article{graisRaw2018,
  title = {Raw {{Multi}}-{{Channel Audio Source Separation}} Using {{Multi}}-{{Resolution Convolutional Auto}}-{{Encoders}}},
  author = {Grais, Emad M. and Ward, Dominic and Plumbley, Mark D.},
  date = {2018-03-01},
  url = {http://arxiv.org/abs/1803.00702},
  urldate = {2020-03-13},
  abstract = {Supervised multi-channel audio source separation requires extracting useful spectral, temporal, and spatial features from the mixed signals. The success of many existing systems is therefore largely dependent on the choice of features used for training. In this work, we introduce a novel multi-channel, multi-resolution convolutional auto-encoder neural network that works on raw time-domain signals to determine appropriate multi-resolution features for separating the singing-voice from stereo music. Our experimental results show that the proposed method can achieve multi-channel audio source separation without the need for hand-crafted features or any pre- or post-processing.},
  archivePrefix = {arXiv},
  eprint = {1803.00702},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/3TXQBSD2/Grais et al. - 2018 - Raw Multi-Channel Audio Source Separation using Mu.pdf;/home/morris/docs/lib/Zotero/storage/SX4NLYC8/1803.html},
  keywords = {68T01; 68T10; 68T45; 62H25,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia,Computer Science - Sound,H.5.5,I.2,I.2.6,I.4,I.4.3,I.5},
  primaryClass = {cs}
}

@article{graisRaw2018a,
  title = {Raw {{Multi}}-{{Channel Audio Source Separation}} Using {{Multi}}-{{Resolution Convolutional Auto}}-{{Encoders}}},
  author = {Grais, Emad M. and Ward, Dominic and Plumbley, Mark D.},
  date = {2018-03-01},
  url = {http://arxiv.org/abs/1803.00702},
  urldate = {2020-06-24},
  abstract = {Supervised multi-channel audio source separation requires extracting useful spectral, temporal, and spatial features from the mixed signals. The success of many existing systems is therefore largely dependent on the choice of features used for training. In this work, we introduce a novel multi-channel, multi-resolution convolutional auto-encoder neural network that works on raw time-domain signals to determine appropriate multi-resolution features for separating the singing-voice from stereo music. Our experimental results show that the proposed method can achieve multi-channel audio source separation without the need for hand-crafted features or any pre- or post-processing.},
  archivePrefix = {arXiv},
  eprint = {1803.00702},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/T2ZDE7XA/Grais et al. - 2018 - Raw Multi-Channel Audio Source Separation using Mu.pdf},
  keywords = {68T01; 68T10; 68T45; 62H25,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia,Computer Science - Sound,H.5.5,I.2,I.2.6,I.4,I.4.3,I.5},
  primaryClass = {cs}
}

@article{grathwohlYour2019a,
  title = {Your {{Classifier}} Is {{Secretly}} an {{Energy Based Model}} and {{You Should Treat}} It {{Like One}}},
  author = {Grathwohl, Will and Wang, Kuan-Chieh and Jacobsen, Jörn-Henrik and Duvenaud, David and Norouzi, Mohammad and Swersky, Kevin},
  date = {2019-12-11},
  url = {http://arxiv.org/abs/1912.03263},
  urldate = {2020-05-25},
  abstract = {We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x,y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may beused and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, andout-of-distribution detection while also enabling our models to generate samplesrivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and presentan approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-artin both generative and discriminative learning within one hybrid model.},
  archivePrefix = {arXiv},
  eprint = {1912.03263},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/JAGJV2VP/Grathwohl et al. - 2019 - Your Classifier is Secretly an Energy Based Model .pdf},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{greffMultiObject2019,
  title = {Multi-{{Object Representation Learning}} with {{Iterative Variational Inference}}},
  author = {Greff, Klaus and Kaufman, Raphaël Lopez and Kabra, Rishabh and Watters, Nick and Burgess, Chris and Zoran, Daniel and Matthey, Loic and Botvinick, Matthew and Lerchner, Alexander},
  date = {2019-03-01},
  url = {http://arxiv.org/abs/1903.00450},
  urldate = {2019-10-07},
  abstract = {Human perception is structured around objects which form the basis for our higher-level cognition and impressive systematic generalization abilities. Yet most work on representation learning focuses on feature learning without even considering multiple objects, or treats segmentation as an (often supervised) preprocessing step. Instead, we argue for the importance of learning to segment and represent objects jointly. We demonstrate that, starting from the simple assumption that a scene is composed of multiple entities, it is possible to learn to segment images into interpretable objects with disentangled representations. Our method learns -- without supervision -- to inpaint occluded parts, and extrapolates to scenes with more objects and to unseen objects with novel feature combinations. We also show that, due to the use of iterative variational inference, our system is able to learn multi-modal posteriors for ambiguous inputs and extends naturally to sequences.},
  archivePrefix = {arXiv},
  eprint = {1903.00450},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/P4KJQ6ZN/Greff et al. - 2019 - Multi-Object Representation Learning with Iterativ.pdf;/home/morris/docs/lib/Zotero/storage/PACXRM24/1903.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{gutmannNoisecontrastive2010,
  title = {Noise-Contrastive Estimation: {{A}} New Estimation Principle for Unnormalized Statistical Models},
  shorttitle = {Noise-Contrastive Estimation},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Gutmann, Michael and Hyvärinen, Aapo},
  date = {2010-03-31},
  pages = {297--304},
  url = {http://proceedings.mlr.press/v9/gutmann10a.html},
  urldate = {2019-10-09},
  abstract = {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially gene...},
  eventtitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  file = {/home/morris/docs/lib/Zotero/storage/7LCH46YZ/gutmann10a.html},
  langid = {english}
}

@article{higginsBetaVAE2016,
  title = {Beta-{{VAE}}: {{Learning Basic Visual Concepts}} with a {{Constrained Variational Framework}}},
  shorttitle = {Beta-{{VAE}}},
  author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  date = {2016-11-04},
  url = {https://openreview.net/forum?id=Sy2fzU9gl},
  urldate = {2020-03-08},
  abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial...},
  file = {/home/morris/docs/lib/Zotero/storage/R9EBSMT9/Higgins et al. - 2016 - beta-VAE Learning Basic Visual Concepts with a Co.pdf;/home/morris/docs/lib/Zotero/storage/KLB9QBRT/forum.html}
}

@article{hochreiterLong1997a,
  title = {Long {{Short}}-{{Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date = {1997-11-01},
  journaltitle = {Neural Computation},
  volume = {9},
  pages = {1735--1780},
  publisher = {{MIT Press}},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  url = {https://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
  urldate = {2020-03-08},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  file = {/home/morris/docs/lib/Zotero/storage/BSUZIN7R/neco.1997.9.8.html},
  number = {8}
}

@article{hochsteinView2002,
  title = {View from the Top: Hierarchies and Reverse Hierarchies in the Visual System},
  shorttitle = {View from the Top},
  author = {Hochstein, Shaul and Ahissar, Merav},
  date = {2002-12-05},
  journaltitle = {Neuron},
  volume = {36},
  pages = {791--804},
  issn = {0896-6273},
  doi = {10.1016/s0896-6273(02)01091-7},
  abstract = {We propose that explicit vision advances in reverse hierarchical direction, as shown for perceptual learning. Processing along the feedforward hierarchy of areas, leading to increasingly complex representations, is automatic and implicit, while conscious perception begins at the hierarchy's top, gradually returning downward as needed. Thus, our initial conscious percept--vision at a glance--matches a high-level, generalized, categorical scene interpretation, identifying "forest before trees." For later vision with scrutiny, reverse hierarchy routines focus attention to specific, active, low-level units, incorporating into conscious perception detailed information available there. Reverse Hierarchy Theory dissociates between early explicit perception and implicit low-level vision, explaining a variety of phenomena. Feature search "pop-out" is attributed to high areas, where large receptive fields underlie spread attention detecting categorical differences. Search for conjunctions or fine discriminations depends on reentry to low-level specific receptive fields using serial focused attention, consistent with recently reported primary visual cortex effects.},
  eprint = {12467584},
  eprinttype = {pmid},
  file = {/home/morris/docs/lib/Zotero/storage/DDAQM7G7/Hochstein and Ahissar - 2002 - View from the top hierarchies and reverse hierarc.pdf},
  keywords = {Humans,Learning,Neurons,Neuropsychological Tests,out,Pattern Recognition; Visual,Vision; Ocular,Visual Cortex,Visual Perception},
  langid = {english},
  number = {5}
}

@inproceedings{holschneiderRealTime1990,
  title = {A {{Real}}-{{Time Algorithm}} for {{Signal Analysis}} with the {{Help}} of the {{Wavelet Transform}}},
  booktitle = {Wavelets},
  author = {Holschneider, M. and Kronland-Martinet, R. and Morlet, J. and Tchamitchian, Ph.},
  editor = {Combes, Jean-Michel and Grossmann, Alexander and Tchamitchian, Philippe},
  date = {1990},
  pages = {286--297},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-75988-8_28},
  abstract = {The purpose of this paper is to present a real-time algorithm for the analysis of time-varying signals with the help of the wavelet transform. We shall briefly describe this transformation in the following. For more details, we refer to the literature [1].},
  file = {/home/morris/docs/lib/Zotero/storage/4RW3KTQY/Holschneider et al. - 1990 - A Real-Time Algorithm for Signal Analysis with the.pdf},
  isbn = {978-3-642-75988-8},
  keywords = {Dilation Parameter,Discrete Wavelet,Graph Algebra,Interpolation Filter,Wavelet Transform},
  langid = {english},
  series = {Inverse Problems and Theoretical Imaging}
}

@article{ilseDIVA2019,
  title = {{{DIVA}}: {{Domain Invariant Variational Autoencoders}}},
  shorttitle = {{{DIVA}}},
  author = {Ilse, Maximilian and Tomczak, Jakub M. and Louizos, Christos and Welling, Max},
  date = {2019-05-24},
  url = {http://arxiv.org/abs/1905.10427},
  urldate = {2019-10-07},
  abstract = {We consider the problem of domain generalization, namely, how to learn representations given data from a set of domains that generalize to data from a previously unseen domain. We propose the Domain Invariant Variational Autoencoder (DIVA), a generative model that tackles this problem by learning three independent latent subspaces, one for the domain, one for the class, and one for any residual variations. We highlight that due to the generative nature of our model we can also incorporate unlabeled data from known or previously unseen domains. To the best of our knowledge this has not been done before in a domain generalization setting. This property is highly desirable in fields like medical imaging where labeled data is scarce. We experimentally evaluate our model on the rotated MNIST benchmark and a malaria cell images dataset where we show that (i) the learned subspaces are indeed complementary to each other, (ii) we improve upon recent works on this task and (iii) incorporating unlabelled data can boost the performance even further.},
  archivePrefix = {arXiv},
  eprint = {1905.10427},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/YARZQKCP/Ilse et al. - 2019 - DIVA Domain Invariant Variational Autoencoders.pdf;/home/morris/docs/lib/Zotero/storage/UD3BN9XV/1905.html},
  keywords = {Computer Science - Machine Learning,look,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@report{iso/tc43acousticsISO1975,
  title = {{{ISO}} 16 – {{Acoustics}} — {{Standard}} Tuning Frequency},
  author = {{ISO/TC 43 Acoustics}},
  date = {1975},
  url = {https://www.iso.org/standard/3601.html},
  abstract = {Specifies the frequency for the note A in the treble stave and shall be 440 Hz. Tuning and retuning shall be effected by instruments producing it within an accuracy of 0,5 Hz.}
}

@book{janschnuppAuditory2010,
  title = {Auditory {{Neuroscience}}},
  author = {{Jan Schnupp} and {Israel Nelken} and {Andrew J. King}},
  date = {2010},
  publisher = {{MIT Press}},
  abstract = {An integrated overview of hearing and the interplay of physical, biological, and psychological processes underlying it.},
  file = {/home/morris/docs/lib/Zotero/storage/BAVM5JM4/Jan Schnupp et al. - 2010 - Auditory Neuroscience.pdf},
  isbn = {978-0-262-11318-2},
  langid = {american},
  pagetotal = {366}
}

@inproceedings{janssonSinging2017,
  title = {Singing {{Voice Separation}} with {{Deep U}}-{{Net Convolutional Networks}}},
  booktitle = {{{ISMIR}}},
  author = {Jansson, Andreas and Humphrey, Eric J. and Montecchio, Nicola and Bittner, Rachel M. and Kumar, Aparna and Weyde, Tillman},
  date = {2017},
  abstract = {The decomposition of a music audio signal into its vocal and backing track components is analogous to image-to-image translation, where a mixed spectrogram is transformed into its constituent sources. We propose a novel application of the U-Net architecture — initially developed for medical imaging — for the task of source separation, given its proven capacity for recreating the fine, low-level detail required for high-quality audio reproduction. Through both quantitative evaluation and subjective assessment, experiments demonstrate that the proposed algorithm achieves state-of-the-art performance.},
  file = {/home/morris/docs/lib/Zotero/storage/QVLSE2MD/Jansson et al. - 2017 - Singing Voice Separation with Deep U-Net Convoluti.pdf}
}

@article{jayaramSource2020,
  title = {Source {{Separation}} with {{Deep Generative Priors}}},
  author = {Jayaram, Vivek and Thickstun, John},
  date = {2020-02-18},
  url = {http://arxiv.org/abs/2002.07942},
  urldate = {2020-02-28},
  abstract = {Despite substantial progress in signal source separation, results for richly structured data continue to contain perceptible artifacts. In contrast, recent deep generative models can produce authentic samples in a variety of domains that are indistinguishable from samples of the data distribution. This paper introduces a Bayesian approach to source separation that uses generative models as priors over the components of a mixture of sources, and Langevin dynamics to sample from the posterior distribution of sources given a mixture. This decouples the source separation problem from generative modeling, enabling us to directly use cutting-edge generative models as priors. The method achieves state-of-the-art performance for MNIST digit separation. We introduce new methodology for evaluating separation quality on richer datasets, providing quantitative evaluation of separation results on CIFAR-10. We also provide qualitative results on LSUN.},
  archivePrefix = {arXiv},
  eprint = {2002.07942},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/IHJPBCKB/Jayaram and Thickstun - 2020 - Source Separation with Deep Generative Priors.pdf;/home/morris/docs/lib/Zotero/storage/L977BPFJ/2002.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@unpublished{jordanb.l.smithMultiPart2017,
  title = {Multi-{{Part Pattern Analysis}}: {{Combining Structure Analysis}} and {{Source Separation}} to {{Discover Intra}}-{{Part Repeated Sequences}}.},
  shorttitle = {Multi-{{Part Pattern Analysis}}},
  author = {{Jordan B. L. Smith} and {Masataka Goto}},
  date = {2017-10-23},
  url = {https://zenodo.org/record/1417685},
  urldate = {2019-10-11},
  abstract = {[TODO] Add abstract here.},
  file = {/home/morris/docs/lib/Zotero/storage/2VLK28ZF/Jordan B. L. Smith and Masataka Goto - 2017 - Multi-Part Pattern Analysis Combining Structure A.pdf},
  venue = {{Suzhou, China}}
}

@article{jordanIntroduction1999,
  title = {An {{Introduction}} to {{Variational Methods}} for {{Graphical Models}}},
  author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
  date = {1999-11-01},
  journaltitle = {Machine Learning},
  volume = {37},
  pages = {183--233},
  issn = {1573-0565},
  doi = {10.1023/A:1007665907178},
  url = {https://doi.org/10.1023/A:1007665907178},
  urldate = {2020-03-08},
  abstract = {This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.},
  file = {/home/morris/docs/lib/Zotero/storage/DGE97QD9/Jordan et al. - 1999 - An Introduction to Variational Methods for Graphic.pdf},
  langid = {english},
  number = {2}
}

@unpublished{juans.gomezJazz2018,
  title = {Jazz {{Solo Instrument Classification}} with {{Convolutional Neural Networks}}, {{Source Separation}}, and {{Transfer Learning}}},
  author = {{Juan S. Gómez} and {Jakob Abeßer} and {Estefanía Cano}},
  date = {2018-09-23},
  url = {https://zenodo.org/record/1492481},
  urldate = {2019-10-10},
  abstract = {Predominant instrument recognition in ensemble recordings remains a challenging task, particularly if closelyrelated instruments such as alto and tenor saxophone need to be distinguished. In this paper, we build upon a recentlyproposed instrument recognition algorithm based on a hybrid deep neural network: a combination of convolutional and fully connected layers for learning characteristic spectral-temporal patterns. We systematically evaluate harmonic/percussive and solo/accompaniment source separation algorithms as pre-processing steps to reduce the overlap among multiple instruments prior to the instrument recognition step. For the particular use-case of solo instrument recognition in jazz ensemble recordings, we further apply transfer learning techniques to fine-tune a previously trained instrument recognition model for classifying six jazz solo instruments. Our results indicate that both source separation as pre-processing step as well as transfer learning clearly improve recognition performance, especially for smaller subsets of highly similar instruments.},
  venue = {{Paris, France}}
}

@article{kalchbrennerEfficient2018,
  title = {Efficient {{Neural Audio Synthesis}}},
  author = {Kalchbrenner, Nal and Elsen, Erich and Simonyan, Karen and Noury, Seb and Casagrande, Norman and Lockhart, Edward and Stimberg, Florian and van den Oord, Aäron and Dieleman, Sander and Kavukcuoglu, Koray},
  date = {2018-02-23},
  url = {http://arxiv.org/abs/1802.08435},
  urldate = {2019-10-17},
  abstract = {Sequential models achieve state-of-the-art results in audio, visual and textual domains with respect to both estimating the data distribution and generating high-quality samples. Efficient sampling for this class of models has however remained an elusive problem. With a focus on text-to-speech synthesis, we describe a set of general techniques for reducing sampling time while maintaining high output quality. We first describe a single-layer recurrent neural network, the WaveRNN, with a dual softmax layer that matches the quality of the state-of-the-art WaveNet model. The compact form of the network makes it possible to generate 24kHz 16-bit audio 4x faster than real time on a GPU. Second, we apply a weight pruning technique to reduce the number of weights in the WaveRNN. We find that, for a constant number of parameters, large sparse networks perform better than small dense networks and this relationship holds for sparsity levels beyond 96\%. The small number of weights in a Sparse WaveRNN makes it possible to sample high-fidelity audio on a mobile CPU in real time. Finally, we propose a new generation scheme based on subscaling that folds a long sequence into a batch of shorter sequences and allows one to generate multiple samples at once. The Subscale WaveRNN produces 16 samples per step without loss of quality and offers an orthogonal method for increasing sampling efficiency.},
  archivePrefix = {arXiv},
  eprint = {1802.08435},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/EC5BAE9L/Kalchbrenner et al. - 2018 - Efficient Neural Audio Synthesis.pdf;/home/morris/docs/lib/Zotero/storage/XP4XQTG3/1802.html},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  options = {useprefix=true},
  primaryClass = {cs, eess}
}

@article{kaspersenHydraNet2019,
  title = {{{HydraNet}}: {{A Network For Singing Voice Separation}}},
  shorttitle = {{{HydraNet}}},
  author = {Kaspersen, Esbern Torgard},
  date = {2019},
  file = {/home/morris/docs/lib/Zotero/storage/NPRXLTRZ/Kaspersen - 2019 - HydraNet A Network For Singing Voice Separation.pdf}
}

@article{kimFloWaveNet2019a,
  title = {{{FloWaveNet}} : {{A Generative Flow}} for {{Raw Audio}}},
  shorttitle = {{{FloWaveNet}}},
  author = {Kim, Sungwon and Lee, Sang-gil and Song, Jongyoon and Kim, Jaehyeon and Yoon, Sungroh},
  date = {2019-05-20},
  url = {http://arxiv.org/abs/1811.02155},
  urldate = {2020-03-07},
  abstract = {Most modern text-to-speech architectures use a WaveNet vocoder for synthesizing high-fidelity waveform audio, but there have been limitations, such as high inference time, in its practical application due to its ancestral sampling scheme. The recently suggested Parallel WaveNet and ClariNet have achieved real-time audio synthesis capability by incorporating inverse autoregressive flow for parallel sampling. However, these approaches require a two-stage training pipeline with a well-trained teacher network and can only produce natural sound by using probability distillation along with auxiliary loss terms. We propose FloWaveNet, a flow-based generative model for raw audio synthesis. FloWaveNet requires only a single-stage training procedure and a single maximum likelihood loss, without any additional auxiliary terms, and it is inherently parallel due to the characteristics of generative flow. The model can efficiently sample raw audio in real-time, with clarity comparable to previous two-stage parallel models. The code and samples for all models, including our FloWaveNet, are publicly available.},
  archivePrefix = {arXiv},
  eprint = {1811.02155},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/2V8KAPQI/Kim et al. - 2019 - FloWaveNet  A Generative Flow for Raw Audio.pdf;/home/morris/docs/lib/Zotero/storage/SMUJRUCV/1811.html},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{kingmaAdam2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-29},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2020-07-15},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archivePrefix = {arXiv},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/8X6GWUIB/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{kingmaAutoEncoding2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2014-05-01},
  url = {http://arxiv.org/abs/1312.6114},
  urldate = {2020-03-08},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archivePrefix = {arXiv},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/LEIDTBDL/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf;/home/morris/docs/lib/Zotero/storage/SBUBUIWB/1312.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{kingmaGlow2018,
  title = {Glow: {{Generative Flow}} with {{Invertible}} 1x1 {{Convolutions}}},
  shorttitle = {Glow},
  author = {Kingma, Diederik P. and Dhariwal, Prafulla},
  date = {2018-07-10},
  url = {http://arxiv.org/abs/1807.03039},
  urldate = {2019-11-11},
  abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow},
  archivePrefix = {arXiv},
  eprint = {1807.03039},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/NEICYRZQ/Kingma and Dhariwal - 2018 - Glow Generative Flow with Invertible 1x1 Convolut.pdf;/home/morris/docs/lib/Zotero/storage/33ZWAQF9/1807.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{kingmaImproving2016,
  title = {Improving {{Variational Inference}} with {{Inverse Autoregressive Flow}}},
  author = {Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  date = {2016-06-15},
  url = {http://arxiv.org/abs/1606.04934},
  urldate = {2019-10-17},
  abstract = {The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
  archivePrefix = {arXiv},
  eprint = {1606.04934},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/EY8RSSJB/Kingma et al. - 2016 - Improving Variational Inference with Inverse Autor.pdf;/home/morris/docs/lib/Zotero/storage/JQZBR25T/1606.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{kingmaIntroduction2019,
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2019-07-24},
  url = {http://arxiv.org/abs/1906.02691},
  urldate = {2019-11-09},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  archivePrefix = {arXiv},
  eprint = {1906.02691},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/ID2DSLRV/Kingma and Welling - 2019 - An Introduction to Variational Autoencoders.pdf;/home/morris/docs/lib/Zotero/storage/EH5KVA56/1906.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{kingmaSemiSupervised2014,
  title = {Semi-{{Supervised Learning}} with {{Deep Generative Models}}},
  author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
  date = {2014-10-31},
  url = {http://arxiv.org/abs/1406.5298},
  urldate = {2019-12-03},
  abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
  archivePrefix = {arXiv},
  eprint = {1406.5298},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/FRZAPM99/Kingma et al. - 2014 - Semi-Supervised Learning with Deep Generative Mode.pdf;/home/morris/docs/lib/Zotero/storage/A6UXX6CA/1406.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{kotelnikovCarrying1933,
  title = {On the Carrying Capacity of the Ether and Wire in Telecommunications},
  booktitle = {Material for the {{First All}}-{{Union Conference}} on {{Questions}} of {{Communication}}},
  author = {Kotelnikov, Vladimir},
  date = {1933},
  publisher = {{Izd. Red. Upr. Svyazi RKKA}},
  location = {{Moscow}}
}

@unpublished{li-chiayangMidiNet2017,
  title = {{{MidiNet}}: {{A Convolutional Generative Adversarial Network}} for {{Symbolic}}-{{Domain Music Generation}}.},
  shorttitle = {{{MidiNet}}},
  author = {{Li-Chia Yang} and {Szu-Yu Chou} and {Yi-Hsuan Yang}},
  date = {2017-10-23},
  url = {https://zenodo.org/record/1415990},
  urldate = {2019-10-11},
  abstract = {[TODO] Add abstract here.},
  file = {/home/morris/docs/lib/Zotero/storage/LWSNJYL4/Li-Chia Yang et al. - 2017 - MidiNet A Convolutional Generative Adversarial Ne.pdf},
  venue = {{Suzhou, China}}
}

@article{lluisEndtoend2019,
  title = {End-to-End Music Source Separation: Is It Possible in the Waveform Domain?},
  shorttitle = {End-to-End Music Source Separation},
  author = {Lluís, Francesc and Pons, Jordi and Serra, Xavier},
  date = {2019-06-28},
  url = {http://arxiv.org/abs/1810.12187},
  urldate = {2019-11-07},
  abstract = {Most of the currently successful source separation techniques use the magnitude spectrogram as input, and are therefore by default omitting part of the signal: the phase. To avoid omitting potentially useful information, we study the viability of using end-to-end models for music source separation --- which take into account all the information available in the raw audio signal, including the phase. Although during the last decades end-to-end music source separation has been considered almost unattainable, our results confirm that waveform-based models can perform similarly (if not better) than a spectrogram-based deep learning model. Namely: a Wavenet-based model we propose and Wave-U-Net can outperform DeepConvSep, a recent spectrogram-based deep learning model.},
  archivePrefix = {arXiv},
  eprint = {1810.12187},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/FX7L75GH/Lluís et al. - 2019 - End-to-end music source separation is it possible.pdf;/home/morris/docs/lib/Zotero/storage/AVI56XL2/1810.html},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{locatelloChallenging2019,
  title = {Challenging {{Common Assumptions}} in the {{Unsupervised Learning}} of {{Disentangled Representations}}},
  author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Rätsch, Gunnar and Gelly, Sylvain and Schölkopf, Bernhard and Bachem, Olivier},
  date = {2019-06-18},
  url = {http://arxiv.org/abs/1811.12359},
  urldate = {2019-10-29},
  abstract = {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties ``encouraged'' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.},
  archivePrefix = {arXiv},
  eprint = {1811.12359},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/LX8EJ5U7/Locatello et al. - 2019 - Challenging Common Assumptions in the Unsupervised.pdf;/home/morris/docs/lib/Zotero/storage/KF7KS3R8/1811.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{locatelloWeaklySupervised2020,
  title = {Weakly-{{Supervised Disentanglement Without Compromises}}},
  author = {Locatello, Francesco and Poole, Ben and Rätsch, Gunnar and Schölkopf, Bernhard and Bachem, Olivier and Tschannen, Michael},
  date = {2020-02-07},
  url = {http://arxiv.org/abs/2002.02886},
  urldate = {2020-02-10},
  abstract = {Intelligent agents should be able to learn useful representations by observing changes in their environment. We model such observations as pairs of non-i.i.d. images sharing at least one of the underlying factors of variation. First, we theoretically show that only knowing how many factors have changed, but not which ones, is sufficient to learn disentangled representations. Second, we provide practical algorithms that learn disentangled representations from pairs of images without requiring annotation of groups, individual factors, or the number of factors that have changed. Third, we perform a large-scale empirical study and show that such pairs of observations are sufficient to reliably learn disentangled representations on several benchmark data sets. Finally, we evaluate our learned representations and find that they are simultaneously useful on a diverse suite of tasks, including generalization under covariate shifts, fairness, and abstract reasoning. Overall, our results demonstrate that weak supervision enables learning of useful disentangled representations in realistic scenarios.},
  archivePrefix = {arXiv},
  eprint = {2002.02886},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/TL4DXRV3/Locatello et al. - 2020 - Weakly-Supervised Disentanglement Without Compromi.pdf;/home/morris/docs/lib/Zotero/storage/4UCN8RHF/2002.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@unpublished{manzelliConditioning2018,
  title = {Conditioning {{Deep Generative Raw Audio Models}} for {{Structured Automatic Music}}},
  author = {Manzelli, Rachel and Thakkar, Vijay and Siahkamari, Ali and Kulis, Brian},
  date = {2018-09-23},
  url = {https://zenodo.org/record/1492375},
  urldate = {2019-10-10},
  abstract = {Existing automatic music generation approaches that feature deep learning can be broadly classified into two types: raw audio models and symbolic models. Symbolic models, which train and generate at the note level, are currently the more prevalent approach; these models can capture long-range dependencies of melodic structure, but fail to grasp the nuances and richness of raw audio generations. Raw audio models, such as DeepMind's WaveNet, train directly on sampled audio waveforms, allowing them to produce realistic-sounding, albeit unstructured music. In this paper, we propose an automatic music generation methodology combining both of these approaches to create structured, realistic-sounding compositions. We consider a Long Short Term Memory network to learn the melodic structure of different styles of music, and then use the unique symbolic generations from this model as a conditioning input to a WaveNet-based raw audio generator, creating a model for automatic, novel music. We then evaluate this approach by showcasing results of this work.},
  file = {/home/morris/docs/lib/Zotero/storage/6VKRRSP6/Rachel Manzelli et al. - 2018 - Conditioning Deep Generative Raw Audio Models for .pdf},
  venue = {{Paris, France}}
}

@unpublished{mariusmironMonaural2017,
  title = {Monaural {{Score}}-{{Informed Source Separation}} for {{Classical Music Using Convolutional Neural Networks}}.},
  author = {{Marius Miron} and {Jordi Janer} and {Emilia Gómez}},
  date = {2017-10-23},
  url = {https://zenodo.org/record/1416498},
  urldate = {2019-10-11},
  abstract = {[TODO] Add abstract here.},
  file = {/home/morris/docs/lib/Zotero/storage/SBN2X82X/Marius Miron et al. - 2017 - Monaural Score-Informed Source Separation for Clas.pdf},
  keywords = {read},
  venue = {{Suzhou, China}}
}

@article{matsudaEstimation2018,
  title = {Estimation of {{Non}}-{{Normalized Mixture Models}} and {{Clustering Using Deep Representation}}},
  author = {Matsuda, Takeru and Hyvarinen, Aapo},
  date = {2018-05-19},
  url = {http://arxiv.org/abs/1805.07516},
  urldate = {2019-11-05},
  abstract = {We develop a general method for estimating a finite mixture of non-normalized models. Here, a non-normalized model is defined to be a parametric distribution with an intractable normalization constant. Existing methods for estimating non-normalized models without computing the normalization constant are not applicable to mixture models because they contain more than one intractable normalization constant. The proposed method is derived by extending noise contrastive estimation (NCE), which estimates non-normalized models by discriminating between the observed data and some artificially generated noise. We also propose an extension of NCE with multiple noise distributions. Then, based on the observation that conventional classification learning with neural networks is implicitly assuming an exponential family as a generative model, we introduce a method for clustering unlabeled data by estimating a finite mixture of distributions in an exponential family. Estimation of this mixture model is attained by the proposed extensions of NCE where the training data of neural networks are used as noise. Thus, the proposed method provides a probabilistically principled clustering method that is able to utilize a deep representation. Application to image clustering using a deep neural network gives promising results.},
  archivePrefix = {arXiv},
  eprint = {1805.07516},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/PV7PPFC8/Matsuda and Hyvarinen - 2018 - Estimation of Non-Normalized Mixture Models and Cl.pdf;/home/morris/docs/lib/Zotero/storage/RJBRWWBR/1805.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{meseguer-brocalConditionedUNet2019,
  title = {Conditioned-{{U}}-{{Net}}: {{Introducing}} a {{Control Mechanism}} in the {{U}}-{{Net}} for {{Multiple Source Separations}}},
  shorttitle = {Conditioned-{{U}}-{{Net}}},
  author = {Meseguer-Brocal, Gabriel and Peeters, Geoffroy},
  date = {2019-07-09},
  url = {http://arxiv.org/abs/1907.01277},
  urldate = {2019-11-05},
  abstract = {Data-driven models for audio source separation such as U-Net or Wave-U-Net are usually models dedicated to and specifically trained for a single task, e.g. a particular instrument isolation. Training them for various tasks at once commonly results in worse performances than training them for a single specialized task. In this work, we introduce the Conditioned-U-Net (C-U-Net) which adds a control mechanism to the standard U-Net. The control mechanism allows us to train a unique and generic U-Net to perform the separation of various instruments. The C-U-Net decides the instrument to isolate according to a one-hot-encoding input vector. The input vector is embedded to obtain the parameters that control Feature-wise Linear Modulation (FiLM) layers. FiLM layers modify the U-Net feature maps in order to separate the desired instrument via affine transformations. The C-U-Net performs different instrument separations, all with a single model achieving the same performances as the dedicated ones at a lower cost.},
  archivePrefix = {arXiv},
  eprint = {1907.01277},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/JSPMX5AH/Meseguer-Brocal and Peeters - 2019 - Conditioned-U-Net Introducing a Control Mechanism.pdf;/home/morris/docs/lib/Zotero/storage/Y9IIIY9I/1907.html},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{morUniversal2018,
  title = {A {{Universal Music Translation Network}}},
  author = {Mor, Noam and Wolf, Lior and Polyak, Adam and Taigman, Yaniv},
  date = {2018-05-23},
  url = {http://arxiv.org/abs/1805.07848},
  urldate = {2019-10-30},
  abstract = {We present a method for translating music across musical instruments, genres, and styles. This method is based on a multi-domain wavenet autoencoder, with a shared encoder and a disentangled latent space that is trained end-to-end on waveforms. Employing a diverse training dataset and large net capacity, the domain-independent encoder allows us to translate even from musical domains that were not seen during training. The method is unsupervised and does not rely on supervision in the form of matched samples between domains or musical transcriptions. We evaluate our method on NSynth, as well as on a dataset collected from professional musicians, and achieve convincing translations, even when translating from whistling, potentially enabling the creation of instrumental music by untrained humans.},
  archivePrefix = {arXiv},
  eprint = {1805.07848},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/TQZP88U6/Mor et al. - 2018 - A Universal Music Translation Network.pdf;/home/morris/docs/lib/Zotero/storage/ELWCQVNN/1805.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Sound,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{nahumLowLevel2008,
  title = {Low-{{Level Information}} and {{High}}-{{Level Perception}}: {{The Case}} of {{Speech}} in {{Noise}}},
  shorttitle = {Low-{{Level Information}} and {{High}}-{{Level Perception}}},
  author = {Nahum, Mor and Nelken, Israel and Ahissar, Merav},
  date = {2008-05-20},
  journaltitle = {PLOS Biology},
  volume = {6},
  pages = {e126},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.0060126},
  url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.0060126},
  urldate = {2019-11-01},
  abstract = {Auditory information is processed in a fine-to-crude hierarchical scheme, from low-level acoustic information to high-level abstract representations, such as phonological labels. We now ask whether fine acoustic information, which is not retained at high levels, can still be used to extract speech from noise. Previous theories suggested either full availability of low-level information or availability that is limited by task difficulty. We propose a third alternative, based on the Reverse Hierarchy Theory (RHT), originally derived to describe the relations between the processing hierarchy and visual perception. RHT asserts that only the higher levels of the hierarchy are immediately available for perception. Direct access to low-level information requires specific conditions, and can be achieved only at the cost of concurrent comprehension. We tested the predictions of these three views in a series of experiments in which we measured the benefits from utilizing low-level binaural information for speech perception, and compared it to that predicted from a model of the early auditory system. Only auditory RHT could account for the full pattern of the results, suggesting that similar defaults and tradeoffs underlie the relations between hierarchical processing and perception in the visual and auditory modalities.},
  file = {/home/morris/docs/lib/Zotero/storage/5TXW36XH/Nahum et al. - 2008 - Low-Level Information and High-Level Perception T.pdf;/home/morris/docs/lib/Zotero/storage/8XQ3ZVH5/article.html},
  keywords = {Acoustics,Auditory system,Ears,Human performance,Phonology,read,Semantics,Sensory perception,Vision},
  langid = {english},
  number = {5}
}

@article{narayanaswamyAudio2019,
  title = {Audio {{Source Separation}} via {{Multi}}-{{Scale Learning}} with {{Dilated Dense U}}-{{Nets}}},
  author = {Narayanaswamy, Vivek Sivaraman and Katoch, Sameeksha and Thiagarajan, Jayaraman J. and Song, Huan and Spanias, Andreas},
  date = {2019-04-08},
  url = {http://arxiv.org/abs/1904.04161},
  urldate = {2020-06-24},
  abstract = {Modern audio source separation techniques rely on optimizing sequence model architectures such as, 1D-CNNs, on mixture recordings to generalize well to unseen mixtures. Specifically, recent focus is on time-domain based architectures such as Wave-U-Net which exploit temporal context by extracting multi-scale features. However, the optimality of the feature extraction process in these architectures has not been well investigated. In this paper, we examine and recommend critical architectural changes that forge an optimal multi-scale feature extraction process. To this end, we replace regular \$1-\$D convolutions with adaptive dilated convolutions that have innate capability of capturing increased context by using large temporal receptive fields. We also investigate the impact of dense connections on the extraction process that encourage feature reuse and better gradient flow. The dense connections between the downsampling and upsampling paths of a U-Net architecture capture multi-resolution information leading to improved temporal modelling. We evaluate the proposed approaches on the MUSDB test dataset. In addition to providing an improved performance over the state-of-the-art, we also provide insights on the impact of different architectural choices on complex data-driven solutions for source separation.},
  archivePrefix = {arXiv},
  eprint = {1904.04161},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/282XL74P/Narayanaswamy et al. - 2019 - Audio Source Separation via Multi-Scale Learning w.pdf},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@software{nativeinstrumentsStem,
  title = {Stem Audio Format},
  author = {{Native Instruments}},
  url = {https://www.stems-music.com/}
}

@article{nealMCMC2012,
  title = {{{MCMC}} Using {{Hamiltonian}} Dynamics},
  author = {Neal, Radford M.},
  date = {2012-06-08},
  url = {http://arxiv.org/abs/1206.1901},
  urldate = {2020-06-22},
  abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard-to-compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, I discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for deciding on acceptance or rejection, computing trajectories using fast approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories from taking much computation time.},
  archivePrefix = {arXiv},
  eprint = {1206.1901},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/4967WDGS/Neal - 2012 - MCMC using Hamiltonian dynamics.pdf},
  keywords = {Physics - Computational Physics,Statistics - Computation},
  primaryClass = {physics, stat}
}

@article{paineFast2016,
  title = {Fast {{Wavenet Generation Algorithm}}},
  author = {Paine, Tom Le and Khorrami, Pooya and Chang, Shiyu and Zhang, Yang and Ramachandran, Prajit and Hasegawa-Johnson, Mark A. and Huang, Thomas S.},
  date = {2016-11-28},
  url = {http://arxiv.org/abs/1611.09482},
  urldate = {2019-11-08},
  abstract = {This paper presents an efficient implementation of the Wavenet generation process called Fast Wavenet. Compared to a naive implementation that has complexity O(2\^L) (L denotes the number of layers in the network), our proposed approach removes redundant convolution operations by caching previous calculations, thereby reducing the complexity to O(L) time. Timing experiments show significant advantages of our fast implementation over a naive one. While this method is presented for Wavenet, the same scheme can be applied anytime one wants to perform autoregressive generation or online prediction using a model with dilated convolution layers. The code for our method is publicly available.},
  archivePrefix = {arXiv},
  eprint = {1611.09482},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/I8MMUJEJ/Paine et al. - 2016 - Fast Wavenet Generation Algorithm.pdf;/home/morris/docs/lib/Zotero/storage/BR76HN7X/1611.html},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Computer Science - Sound},
  primaryClass = {cs}
}

@article{pingWaveFlow2020,
  title = {{{WaveFlow}}: {{A Compact Flow}}-Based {{Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveFlow}}},
  author = {Ping, Wei and Peng, Kainan and Zhao, Kexin and Song, Zhao},
  date = {2020-02-13},
  url = {http://arxiv.org/abs/1912.01219},
  urldate = {2020-03-09},
  abstract = {In this work, we propose WaveFlow, a small-footprint generative flow for raw audio, which is directly trained with maximum likelihood. It handles the long-range structure of waveform with a dilated 2-D convolutional architecture, while modeling the local variations using expressive autoregressive functions. WaveFlow provides a unified view of likelihood-based models for raw audio, including WaveNet and WaveGlow as special cases. It generates high-fidelity speech as WaveNet, while synthesizing several orders of magnitude faster as it only requires a few sequential steps to generate very long waveforms. Furthermore, it can significantly reduce the likelihood gap that has existed between autoregressive models and flow-based models for efficient synthesis. Finally, our small-footprint WaveFlow has only 5.91M parameters, which is 15\$\textbackslash times\$ smaller than WaveGlow. It can generate 22.05 kHz high-fidelity audio 42.6\$\textbackslash times\$ faster than real-time on a V100 GPU without engineered inference kernels.},
  archivePrefix = {arXiv},
  eprint = {1912.01219},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/UZS2MEK7/Ping et al. - 2020 - WaveFlow A Compact Flow-based Model for Raw Audio.pdf;/home/morris/docs/lib/Zotero/storage/WVY93IPG/1912.html},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{polyakAcceleration1992,
  title = {Acceleration of {{Stochastic Approximation}} by {{Averaging}}},
  author = {Polyak, B. T. and Juditsky, A. B.},
  date = {1992-07},
  journaltitle = {SIAM J. Control Optim.},
  volume = {30},
  pages = {838--855},
  issn = {0363-0129},
  doi = {10.1137/0330046},
  url = {http://dx.doi.org/10.1137/0330046},
  urldate = {2019-10-17},
  file = {/home/morris/docs/lib/Zotero/storage/RLZVHHWN/Polyak and Juditsky - 1992 - Acceleration of Stochastic Approximation by Averag.pdf},
  keywords = {optimal algorithms,recursive estimation,stochastic approximation,stochastic optimization},
  number = {4}
}

@inproceedings{polyakAttentionbased2019,
  title = {Attention-Based {{Wavenet Autoencoder}} for {{Universal Voice Conversion}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Polyak, Adam and Wolf, Lior},
  date = {2019-05},
  pages = {6800--6804},
  issn = {2379-190X, 1520-6149},
  doi = {10.1109/ICASSP.2019.8682589},
  abstract = {We present a method for converting any voice to a target voice. The method is based on a WaveNet autoencoder, with the addition of a novel attention component that supports the modification of timing between the input and the output samples. Training the attention is done in an unsupervised way, by teaching the neural network to recover the original timing from an artificially modified one. Adding a generic voice robot, which we convert to the target voice, we present a robust Text To Speech pipeline that is able to train without any transcript. Our experiments show that the proposed method is able to recover the timing of the speaker and that the proposed pipeline provides a competitive Text To Speech method.},
  eventtitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  file = {/home/morris/docs/lib/Zotero/storage/HBGV67VQ/polyak2019.pdf;/home/morris/docs/lib/Zotero/storage/64I5YBJT/8682589.html},
  keywords = {attention component,attention-based wavenet autoencoder,Convolution,Decoding,Encoding,generic voice robot,learning (artificial intelligence),neural nets,neural network,Pipelines,Robots,speaker recognition,speech synthesis,target voice,Text to Speech method,Timing,Training,universal voice conversion}
}

@article{ponsRandomly2019,
  title = {Randomly Weighted {{CNNs}} for (Music) Audio Classification},
  author = {Pons, Jordi and Serra, Xavier},
  date = {2019-02-14},
  url = {http://arxiv.org/abs/1805.00237},
  urldate = {2020-03-13},
  abstract = {The computer vision literature shows that randomly weighted neural networks perform reasonably as feature extractors. Following this idea, we study how non-trained (randomly weighted) convolutional neural networks perform as feature extractors for (music) audio classification tasks. We use features extracted from the embeddings of deep architectures as input to a classifier - with the goal to compare classification accuracies when using different randomly weighted architectures. By following this methodology, we run a comprehensive evaluation of the current deep architectures for audio classification, and provide evidence that the architectures alone are an important piece for resolving (music) audio problems using deep neural networks.},
  archivePrefix = {arXiv},
  eprint = {1805.00237},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/CEUP7ZY6/Pons and Serra - 2019 - Randomly weighted CNNs for (music) audio classific.pdf;/home/morris/docs/lib/Zotero/storage/XQI7W2S3/1805.html},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{prasadanTime,
  title = {Time {{Series Source Separation}} Using {{Dynamic Mode Decomposition}}},
  author = {Prasadan, Arvind and Nadakuditi, Raj Rao},
  doi = {10.5281/zenodo.2656681},
  url = {http://arxiv.org/abs/1903.01310},
  urldate = {2019-11-05},
  abstract = {The dynamic mode decomposition (DMD) extracted dynamic modes are the non-orthogonal eigenvectors of the matrix that best approximates the one-step temporal evolution of the multivariate samples. In the context of dynamic system analysis, the extracted dynamic modes are a generalization of global stability modes. We apply DMD to a data matrix whose rows are linearly independent, additive mixtures of latent time series. We show that when the latent time series are uncorrelated at a lag of one time-step then, in the large sample limit, the recovered dynamic modes will approximate, up to a column-wise normalization, the columns of the mixing matrix. Thus, DMD is a time series blind source separation algorithm in disguise, but is different from closely related second order algorithms such as SOBI and AMUSE. All can unmix mixed ergodic Gaussian time series in a way that kurtosis-based ICA fundamentally cannot. We use our insights on single lag DMD to develop a higher-lag extension, analyze the finite sample performance with and without randomly missing data, and identify settings where the higher lag variant can outperform the conventional single lag variant. We validate our results with numerical simulations, and highlight how DMD can be used in change point detection.},
  archivePrefix = {arXiv},
  eprint = {1903.01310},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/GGMLDGGM/Prasadan and Nadakuditi - Time Series Source Separation using Dynamic Mode D.pdf;/home/morris/docs/lib/Zotero/storage/KTUKI8CG/1903.html},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,out,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{prengerWaveGlow2018,
  title = {{{WaveGlow}}: {{A Flow}}-Based {{Generative Network}} for {{Speech Synthesis}}},
  shorttitle = {{{WaveGlow}}},
  author = {Prenger, Ryan and Valle, Rafael and Catanzaro, Bryan},
  date = {2018-10-30},
  url = {http://arxiv.org/abs/1811.00002},
  urldate = {2019-10-17},
  abstract = {In this paper we propose WaveGlow: a flow-based network capable of generating high quality speech from mel-spectrograms. WaveGlow combines insights from Glow and WaveNet in order to provide fast, efficient and high-quality audio synthesis, without the need for auto-regression. WaveGlow is implemented using only a single network, trained using only a single cost function: maximizing the likelihood of the training data, which makes the training procedure simple and stable. Our PyTorch implementation produces audio samples at a rate of more than 500 kHz on an NVIDIA V100 GPU. Mean Opinion Scores show that it delivers audio quality as good as the best publicly available WaveNet implementation. All code will be made publicly available online.},
  archivePrefix = {arXiv},
  eprint = {1811.00002},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/6NNCHVCF/Prenger et al. - 2018 - WaveGlow A Flow-based Generative Network for Spee.pdf;/home/morris/docs/lib/Zotero/storage/LC9C6FF8/1811.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,look,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@report{Pulse1972,
  title = {Pulse Code Modulation ({{PCM}}) of Voice Frequencies},
  date = {1972},
  institution = {{ITU-T}},
  url = {https://www.itu.int/rec/T-REC-G.711},
  number = {G.711}
}

@data{rafiiMUSDB182017,
  title = {{{MUSDB18}} - a Corpus for Music Separation},
  author = {Rafii, Zafar and Liutkus, Antoine and Stöter, Fabian-Robert and Mimilakis, Stylianos Ioannis and Bittner, Rachel},
  date = {2017-12-17},
  publisher = {{Zenodo}},
  doi = {10.5281/ZENODO.1117372},
  url = {https://zenodo.org/record/1117372},
  urldate = {2020-06-16},
  abstract = {The sigsep musdb18 data set consists of a total of 150 full-track songs of different styles and includes both the stereo mixtures and the original sources, divided between a training subset and a test subset. Its purpose is to serve as a reference database for the design and the evaluation of source separation algorithms. The objective of such signal processing methods is to estimate one or more sources from a set of mixtures, e.g. for karaoke applications. It has been used as the official dataset in the professionally-produced music recordings task for SiSEC 2018, which is the international campaign for the evaluation of source separation algorithms. {$<$}em{$>$}musdb18{$<$}/em{$>$} contains two folders, a folder with a training set: “train”, composed of 100 songs, and a folder with a test set: “test”, composed of 50 songs. Supervised approaches should be trained on the training set and tested on both sets. All files from the {$<$}em{$>$}musdb18{$<$}/em{$>$} dataset are encoded in the Native Instruments stems format (.mp4). It is a multitrack format composed of 5 stereo streams, each one encoded in AAC @256kbps. These signals correspond to: 0 - The mixture, 1 - The drums, 2 - The bass, 3 - The rest of the accompaniment, 4 - The vocals. For each file, the mixture correspond to the sum of all the signals. All signals are stereophonic and encoded at 44.1kHz. As the {$<$}em{$>$}MUSDB18{$<$}/em{$>$} is encoded as STEMS, it relies on ffmpeg to read the multi-stream files. We provide a python wrapper called stempeg that allows to easily parse the dataset and decode the stem tracks on-the-fly. If you use the MUSDB dataset for your research - Cite the MUSDB18 Dataset &lt;code&gt;@misc{MUSDB18, author = {Rafii, Zafar and Liutkus, Antoine and Fabian-Robert St{\"o}ter and Mimilakis, Stylianos Ioannis and Bittner, Rachel}, title = {The {MUSDB18} corpus for music separation}, month = dec, year = 2017, doi = {10.5281/zenodo.1117372}, url = {https://doi.org/10.5281/zenodo.1117372} } &lt;/code&gt; If compare your results with SiSEC 2018 Participants - Cite the SiSEC 2018 LVA/ICA Paper &lt;code&gt;@inproceedings{SiSEC18, author="St{\"o}ter, Fabian-Robert and Liutkus, Antoine and Ito, Nobutaka", title="The 2018 Signal Separation Evaluation Campaign", booktitle="Latent Variable Analysis and Signal Separation: 14th International Conference, LVA/ICA 2018, Surrey, UK", year="2018", pages="293--305" }&lt;/code&gt;},
  keywords = {audio,multitrack,music,source separation,stems},
  version = {1.0.0}
}

@article{razaviGenerating2019,
  title = {Generating {{Diverse High}}-{{Fidelity Images}} with {{VQ}}-{{VAE}}-2},
  author = {Razavi, Ali and van den Oord, Aaron and Vinyals, Oriol},
  date = {2019-06-02},
  url = {http://arxiv.org/abs/1906.00446},
  urldate = {2019-10-17},
  abstract = {We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.},
  archivePrefix = {arXiv},
  eprint = {1906.00446},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/GG26ZUFL/Razavi et al. - 2019 - Generating Diverse High-Fidelity Images with VQ-VA.pdf;/home/morris/docs/lib/Zotero/storage/7SUPNWDY/1906.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,look,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{rethageWavenet2018,
  title = {A {{Wavenet}} for {{Speech Denoising}}},
  author = {Rethage, Dario and Pons, Jordi and Serra, Xavier},
  date = {2018-01-31},
  url = {http://arxiv.org/abs/1706.07162},
  urldate = {2020-03-08},
  abstract = {Currently, most speech processing techniques use magnitude spectrograms as front-end and are therefore by default discarding part of the signal: the phase. In order to overcome this limitation, we propose an end-to-end learning method for speech denoising based on Wavenet. The proposed model adaptation retains Wavenet's powerful acoustic modeling capabilities, while significantly reducing its time-complexity by eliminating its autoregressive nature. Specifically, the model makes use of non-causal, dilated convolutions and predicts target fields instead of a single target sample. The discriminative adaptation of the model we propose, learns in a supervised fashion via minimizing a regression loss. These modifications make the model highly parallelizable during both training and inference. Both computational and perceptual evaluations indicate that the proposed method is preferred to Wiener filtering, a common method based on processing the magnitude spectrogram.},
  archivePrefix = {arXiv},
  eprint = {1706.07162},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/QT3YR5CT/Rethage et al. - 2018 - A Wavenet for Speech Denoising.pdf;/home/morris/docs/lib/Zotero/storage/XG2BIQPA/1706.html},
  keywords = {Computer Science - Sound},
  primaryClass = {cs}
}

@article{rezaabadLearning2020,
  title = {Learning {{Representations}} by {{Maximizing Mutual Information}} in {{Variational Autoencoders}}},
  author = {Rezaabad, Ali Lotfi and Vishwanath, Sriram},
  date = {2020-01-07},
  url = {http://arxiv.org/abs/1912.13361},
  urldate = {2020-01-22},
  abstract = {Variational autoencoders (VAEs) have ushered in a new era of unsupervised learning methods for complex distributions. Although these techniques are elegant in their approach, they are typically not useful for representation learning. In this work, we propose a simple yet powerful class of VAEs that simultaneously result in meaningful learned representations. Our solution is to combine traditional VAEs with mutual information maximization, with the goal to enhance amortized inference in VAEs using Information Theoretic techniques. We call this approach InfoMax-VAE, and such an approach can significantly boost the quality of learned high-level representations. We realize this through the explicit maximization of information measures associated with the representation. Using extensive experiments on varied datasets and setups, we show that InfoMax-VAE outperforms contemporary popular approaches, including Info-VAE and \$\textbackslash beta\$-VAE.},
  archivePrefix = {arXiv},
  eprint = {1912.13361},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/NQ87RFYE/Rezaabad and Vishwanath - 2020 - Learning Representations by Maximizing Mutual Info.pdf;/home/morris/docs/lib/Zotero/storage/ZIF6LLGZ/1912.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{rezendeStochastic2014,
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  date = {2014-05-30},
  url = {http://arxiv.org/abs/1401.4082},
  urldate = {2020-03-08},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
  archivePrefix = {arXiv},
  eprint = {1401.4082},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/Z9EAZQ78/Rezende et al. - 2014 - Stochastic Backpropagation and Approximate Inferen.pdf;/home/morris/docs/lib/Zotero/storage/BYI64P4I/1401.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  primaryClass = {cs, stat}
}

@article{rezendeVariational2016,
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  date = {2016-06-14},
  url = {http://arxiv.org/abs/1505.05770},
  urldate = {2020-03-08},
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  archivePrefix = {arXiv},
  eprint = {1505.05770},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/BHLK5PE2/Rezende and Mohamed - 2016 - Variational Inference with Normalizing Flows.pdf;/home/morris/docs/lib/Zotero/storage/HBEEWEC3/1505.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  primaryClass = {cs, stat}
}

@article{salimansPixelCNN2017,
  title = {{{PixelCNN}}++: {{Improving}} the {{PixelCNN}} with {{Discretized Logistic Mixture Likelihood}} and {{Other Modifications}}},
  shorttitle = {{{PixelCNN}}++},
  author = {Salimans, Tim and Karpathy, Andrej and Chen, Xi and Kingma, Diederik P.},
  date = {2017-01-19},
  url = {http://arxiv.org/abs/1701.05517},
  urldate = {2019-12-02},
  abstract = {PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.},
  archivePrefix = {arXiv},
  eprint = {1701.05517},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/IBDHDU49/Salimans et al. - 2017 - PixelCNN++ Improving the PixelCNN with Discretize.pdf;/home/morris/docs/lib/Zotero/storage/HL38NYGJ/1701.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{shammaEmergence2008,
  title = {On the {{Emergence}} and {{Awareness}} of {{Auditory Objects}}},
  author = {Shamma, Shihab},
  date = {2008-06},
  journaltitle = {PLoS Biol},
  volume = {6},
  issn = {1544-9173},
  doi = {10.1371/journal.pbio.0060155},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2435155/},
  urldate = {2019-11-01},
  abstract = {How do humans successfully navigate the sounds of music and the voice of a friend in the midst of a noisy cocktail party? Two recent articles inPLoS Biology provide psychoacoustic and neuronal clues about where to search for the answers.},
  eprint = {18578570},
  eprinttype = {pmid},
  file = {/home/morris/docs/lib/Zotero/storage/V2U4QYMW/Shamma - 2008 - On the Emergence and Awareness of Auditory Objects.pdf},
  keywords = {out,read},
  number = {6},
  pmcid = {PMC2435155}
}

@article{shuWeakly2019,
  title = {Weakly {{Supervised Disentanglement}} with {{Guarantees}}},
  author = {Shu, Rui and Chen, Yining and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  date = {2019-10-22},
  url = {http://arxiv.org/abs/1910.09772},
  urldate = {2019-10-25},
  abstract = {Learning disentangled representations that correspond to factors of variation in real-world data is critical to interpretable and human-controllable machine learning. Recently, concerns about the viability of learning disentangled representations in a purely unsupervised manner has spurred a shift toward the incorporation of weak supervision. However, there is currently no formalism that identifies when and how weak supervision will guarantee disentanglement. To address this issue, we provide a theoretical framework to assist in analyzing the disentanglement guarantees (or lack thereof) conferred by weak supervision when coupled with learning algorithms based on distribution matching. We empirically verify the guarantees and limitations of several weak supervision methods (restricted labeling, match-pairing, and rank-pairing), demonstrating the predictive power and usefulness of our theoretical framework.},
  archivePrefix = {arXiv},
  eprint = {1910.09772},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/YRGPXPQ4/Shu et al. - 2019 - Weakly Supervised Disentanglement with Guarantees.pdf;/home/morris/docs/lib/Zotero/storage/HW8Z99C8/1910.html},
  keywords = {Computer Science - Machine Learning,look,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{slizovskaiaEndtoEnd2019,
  title = {End-to-{{End Sound Source Separation Conditioned On Instrument Labels}}},
  author = {Slizovskaia, Olga and Kim, Leo and Haro, Gloria and Gomez, Emilia},
  date = {2019-05-09},
  url = {http://arxiv.org/abs/1811.01850},
  urldate = {2020-06-24},
  abstract = {Can we perform an end-to-end music source separation with a variable number of sources using a deep learning model? We present an extension of the Wave-U-Net model which allows end-to-end monaural source separation with a non-fixed number of sources. Furthermore, we propose multiplicative conditioning with instrument labels at the bottleneck of the Wave-U-Net and show its effect on the separation results. This approach leads to other types of conditioning such as audio-visual source separation and score-informed source separation.},
  archivePrefix = {arXiv},
  eprint = {1811.01850},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/M4DLUFYT/Slizovskaia et al. - 2019 - End-to-End Sound Source Separation Conditioned On .pdf},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess},
  version = {2}
}

@unpublished{stollerWaveUNet2018,
  title = {Wave-{{U}}-{{Net}}: {{A Multi}}-{{Scale Neural Network}} for {{End}}-to-{{End Audio Source Separation}}},
  shorttitle = {Wave-{{U}}-{{Net}}},
  author = {Stoller, Daniel and Ewert, Sebastian and Dixon, Simon},
  date = {2018-09-23},
  url = {https://zenodo.org/record/1492417},
  urldate = {2019-10-09},
  abstract = {Models for audio source separation usually operate on the magnitude spectrum, which ignores phase information and makes separation performance dependant on hyperparameters for the spectral front-end. Therefore, we investigate end-to-end source separation in the time-domain, which allows modelling phase information and avoids fixed spectral transformations. Due to high sampling rates for audio, employing a long temporal input context on the sample level is difficult, but required for high quality separation results because of long-range temporal correlations. In this context, we propose the Wave-U-Net, an adaptation of the U-Net to the one-dimensional time domain, which repeatedly resamples feature maps to compute and combine features at different time scales. We introduce further architectural improvements, including an output layer that enforces source additivity, an upsampling technique and a context-aware prediction framework to reduce output artifacts. Experiments for singing voice separation indicate that our architecture yields a performance comparable to a stateof-the-art spectrogram-based U-Net architecture, given the same data. Finally, we reveal a problem with outliers in the currently used SDR evaluation metrics and suggest reporting rank-based statistics to alleviate this problem.},
  file = {/home/morris/docs/lib/Zotero/storage/Z7DJ9M7C/Daniel Stoller et al. - 2018 - Wave-U-Net A Multi-Scale Neural Network for End-t.pdf},
  keywords = {read},
  venue = {{Paris, France}}
}

@article{stoter20182018,
  title = {The 2018 {{Signal Separation Evaluation Campaign}}},
  author = {Stöter, Fabian-Robert and Liutkus, Antoine and Ito, Nobutaka},
  date = {2018-07-06},
  url = {http://arxiv.org/abs/1804.06267},
  urldate = {2019-11-13},
  abstract = {This paper reports the organization and results for the 2018 community-based Signal Separation Evaluation Campaign (SiSEC 2018). This year's edition was focused on audio and pursued the effort towards scaling up and making it easier to prototype audio separation software in an era of machine-learning based systems. For this purpose, we prepared a new music separation database: MUSDB18, featuring close to 10h of audio. Additionally, open-source software was released to automatically load, process and report performance on MUSDB18. Furthermore, a new official Python version for the BSSEval toolbox was released, along with reference implementations for three oracle separation methods: ideal binary mask, ideal ratio mask, and multichannel Wiener filter. We finally report the results obtained by the participants.},
  archivePrefix = {arXiv},
  eprint = {1804.06267},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/YYFI7JNV/Stöter et al. - 2018 - The 2018 Signal Separation Evaluation Campaign.pdf;/home/morris/docs/lib/Zotero/storage/4RB3UFYF/1804.html},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@unpublished{sungheonparkMusic2018,
  title = {Music {{Source Separation Using Stacked Hourglass Networks}}},
  author = {{Sungheon Park} and {Taehoon Kim} and {Kyogu Lee} and {Nojun Kwak}},
  date = {2018-09-23},
  url = {https://zenodo.org/record/1492405},
  urldate = {2019-10-10},
  abstract = {In this paper, we propose a simple yet effective method for multiple music source separation using convolutional neural networks. Stacked hourglass network, which was originally designed for human pose estimation in natural images, is applied to a music source separation task. The network learns features from a spectrogram image across multiple scales and generates masks for each music source. The estimated mask is refined as it passes over stacked hourglass modules. The proposed framework is able to separate multiple music sources using a single network. Experimental results on MIR-1K and DSD100 datasets validate that the proposed method achieves competitive results comparable to the state-of-the-art methods in multiple music source separation and singing voice separation tasks.},
  file = {/home/morris/docs/lib/Zotero/storage/H37YMIHA/Sungheon Park et al. - 2018 - Music Source Separation Using Stacked Hourglass Ne.pdf},
  keywords = {read},
  venue = {{Paris, France}}
}

@article{szegedyGoing2014,
  title = {Going {{Deeper}} with {{Convolutions}}},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  date = {2014-09-16},
  url = {http://arxiv.org/abs/1409.4842},
  urldate = {2020-06-24},
  abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  archivePrefix = {arXiv},
  eprint = {1409.4842},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/7FB4LTWA/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@inproceedings{szegedyGoing2015,
  title = {Going Deeper with Convolutions},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  date = {2015},
  pages = {1--9},
  file = {/home/morris/docs/lib/Zotero/storage/BI7BUXL9/Szegedy et al. - 2015 - Going deeper with convolutions.pdf}
}

@article{tabakFamily2013,
  title = {A family of nonparametric density estimation algorithms},
  author = {Tabak, Esteban and Turner, Cristina V.},
  date = {2013-02},
  journaltitle = {COMMUN. PURE \& APPL. MATHS.},
  volume = {66},
  pages = {145--164},
  publisher = {{Wiley-Liss Inc.}},
  issn = {0010-3640},
  doi = {10.1002/cpa.21423},
  url = {https://nyuscholars.nyu.edu/en/publications/a-family-of-nonparametric-density-estimation-algorithms},
  urldate = {2020-03-09},
  file = {/home/morris/docs/lib/Zotero/storage/5QAREY39/Tabak and Turner - 2013 - A family of nonparametric density estimation algor.pdf;/home/morris/docs/lib/Zotero/storage/CZ9CYX9T/a-family-of-nonparametric-density-estimation-algorithms.html},
  langid = {English (US)},
  number = {2}
}

@article{tschannenRecent2018,
  title = {Recent {{Advances}} in {{Autoencoder}}-{{Based Representation Learning}}},
  author = {Tschannen, Michael and Bachem, Olivier and Lucic, Mario},
  date = {2018-12-12},
  url = {http://arxiv.org/abs/1812.05069},
  urldate = {2019-11-09},
  abstract = {Learning useful representations with little or no supervision is a key challenge in artificial intelligence. We provide an in-depth review of recent advances in representation learning with a focus on autoencoder-based models. To organize these results we make use of meta-priors believed useful for downstream tasks, such as disentanglement and hierarchical organization of features. In particular, we uncover three main mechanisms to enforce such properties, namely (i) regularizing the (approximate or aggregate) posterior distribution, (ii) factorizing the encoding and decoding distribution, or (iii) introducing a structured prior distribution. While there are some promising results, implicit or explicit supervision remains a key enabler and all current methods use strong inductive biases and modeling assumptions. Finally, we provide an analysis of autoencoder-based representation learning through the lens of rate-distortion theory and identify a clear tradeoff between the amount of prior knowledge available about the downstream tasks, and how useful the representation is for this task.},
  archivePrefix = {arXiv},
  eprint = {1812.05069},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/MCV6DPVZ/Tschannen et al. - 2018 - Recent Advances in Autoencoder-Based Representatio.pdf;/home/morris/docs/lib/Zotero/storage/2HLZLG2I/1812.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat},
  version = {1}
}

@inproceedings{uhlichImproving2017,
  title = {Improving Music Source Separation Based on Deep Neural Networks through Data Augmentation and Network Blending},
  booktitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Uhlich, Stefan and Porcu, Marcello and Giron, Franck and Enenkl, Michael and Kemp, Thomas and Takahashi, Naoya and Mitsufuji, Yuki},
  date = {2017-03},
  pages = {261--265},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2017.7952158},
  abstract = {This paper deals with the separation of music into individual instrument tracks which is known to be a challenging problem. We describe two different deep neural network architectures for this task, a feed-forward and a recurrent one, and show that each of them yields themselves state-of-the art results on the SiSEC DSD100 dataset. For the recurrent network, we use data augmentation during training and show that even simple separation networks are prone to overfitting if no data augmentation is used. Furthermore, we propose a blending of both neural network systems where we linearly combine their raw outputs and then perform a multi-channel Wiener filter post-processing. This blending scheme yields the best results that have been reported to-date on the SiSEC DSD100 dataset.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  file = {/home/morris/docs/lib/Zotero/storage/SXJZGLQA/7952158.html},
  keywords = {Blending,blending scheme yields,Context,data augmentation,Deep neural network (DNN),deep neural network architectures,Indexes,Instruments,Long-short term memory (LSTM),mixture models,multichannel Wiener filter post-processing,music,Music source separation (MSS),music source separation improvement,recurrent network,recurrent neural nets,Recurrent neural networks,SiSEC DSD100 dataset,Source separation,speech processing,Training,Wiener filters}
}

@inproceedings{uhlichImproving2017a,
  title = {Improving Music Source Separation Based on Deep Neural Networks through Data Augmentation and Network Blending},
  booktitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Uhlich, Stefan and Porcu, Marcello and Giron, Franck and Enenkl, Michael and Kemp, Thomas and Takahashi, Naoya and Mitsufuji, Yuki},
  date = {2017-03},
  pages = {261--265},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2017.7952158},
  abstract = {This paper deals with the separation of music into individual instrument tracks which is known to be a challenging problem. We describe two different deep neural network architectures for this task, a feed-forward and a recurrent one, and show that each of them yields themselves state-of-the art results on the SiSEC DSD100 dataset. For the recurrent network, we use data augmentation during training and show that even simple separation networks are prone to overfitting if no data augmentation is used. Furthermore, we propose a blending of both neural network systems where we linearly combine their raw outputs and then perform a multi-channel Wiener filter post-processing. This blending scheme yields the best results that have been reported to-date on the SiSEC DSD100 dataset.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  file = {/home/morris/docs/lib/Zotero/storage/4NQUQS45/Uhlich et al. - 2017 - Improving music source separation based on deep ne.pdf},
  keywords = {Blending,blending scheme yields,Context,data augmentation,Deep neural network (DNN),deep neural network architectures,Indexes,Instruments,Long-short term memory (LSTM),mixture models,multichannel Wiener filter post-processing,music,Music source separation (MSS),music source separation improvement,recurrent network,recurrent neural nets,Recurrent neural networks,SiSEC DSD100 dataset,Source separation,speech processing,Training,Wiener filters}
}

@article{vandenoordConditional2016,
  title = {Conditional {{Image Generation}} with {{PixelCNN Decoders}}},
  author = {van den Oord, Aäron and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
  date = {2016-06-16},
  url = {http://arxiv.org/abs/1606.05328},
  urldate = {2019-10-17},
  abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
  archivePrefix = {arXiv},
  eprint = {1606.05328},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/T3VY4I7B/Oord et al. - 2016 - Conditional Image Generation with PixelCNN Decoder.pdf;/home/morris/docs/lib/Zotero/storage/YNL76EFZ/1606.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,look,out},
  options = {useprefix=true},
  primaryClass = {cs}
}

@inproceedings{vandenoordNeural2017,
  title = {Neural {{Discrete Representation Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {van den Oord, Aäron and Vinyals, Oriol and Kavukcuoglu, Koray},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  date = {2017},
  pages = {6306--6315},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/7210-neural-discrete-representation-learning.pdf},
  urldate = {2019-11-14},
  file = {/home/morris/docs/lib/Zotero/storage/CUL4X586/van den Oord et al. - 2017 - Neural Discrete Representation Learning.pdf;/home/morris/docs/lib/Zotero/storage/WDLM7ATT/7210-neural-discrete-representation-learning.html},
  options = {useprefix=true}
}

@article{vandenoordParallel2017,
  title = {Parallel {{WaveNet}}: {{Fast High}}-{{Fidelity Speech Synthesis}}},
  shorttitle = {Parallel {{WaveNet}}},
  author = {van den Oord, Aäron and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and van den Driessche, George and Lockhart, Edward and Cobo, Luis C. and Stimberg, Florian and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman, Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and Hassabis, Demis},
  date = {2017-11-28},
  url = {http://arxiv.org/abs/1711.10433},
  urldate = {2019-10-17},
  abstract = {The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today's massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, and is deployed online by Google Assistant, including serving multiple English and Japanese voices.},
  archivePrefix = {arXiv},
  eprint = {1711.10433},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/G2VX6YTC/Oord et al. - 2017 - Parallel WaveNet Fast High-Fidelity Speech Synthe.pdf;/home/morris/docs/lib/Zotero/storage/DY62RZY8/1711.html},
  keywords = {Computer Science - Machine Learning,read},
  options = {useprefix=true},
  primaryClass = {cs}
}

@article{vandenoordRepresentation2018,
  title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},
  author = {van den Oord, Aäron and Li, Yazhe and Vinyals, Oriol},
  date = {2018-07-10},
  url = {http://arxiv.org/abs/1807.03748},
  urldate = {2019-10-07},
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  archivePrefix = {arXiv},
  eprint = {1807.03748},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/VB7KWS96/Oord et al. - 2018 - Representation Learning with Contrastive Predictiv.pdf;/home/morris/docs/lib/Zotero/storage/T72Y3899/1807.html},
  keywords = {Computer Science - Machine Learning,read,Statistics - Machine Learning},
  options = {useprefix=true},
  primaryClass = {cs, stat}
}

@article{vandenoordWaveNet2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {van den Oord, Aäron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  date = {2016-09-12},
  url = {http://arxiv.org/abs/1609.03499},
  urldate = {2019-10-17},
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  archivePrefix = {arXiv},
  eprint = {1609.03499},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/V2D6WCDY/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf;/home/morris/docs/lib/Zotero/storage/6DDA9CU4/1609.html},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,read},
  options = {useprefix=true},
  primaryClass = {cs}
}

@inproceedings{wellingBayesian2011,
  title = {Bayesian Learning via Stochastic Gradient Langevin Dynamics},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Welling, Max and Teh, Yee Whye},
  date = {2011-06-28},
  pages = {681--688},
  publisher = {{Omnipress}},
  location = {{Bellevue, Washington, USA}},
  abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a "sampling threshold" and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
  file = {/home/morris/docs/lib/Zotero/storage/4YRA5IAB/Welling and Teh - 2011 - Bayesian learning via stochastic gradient langevin.pdf},
  isbn = {978-1-4503-0619-5},
  series = {{{ICML}}'11}
}

@article{yuMultiScale2016,
  title = {Multi-{{Scale Context Aggregation}} by {{Dilated Convolutions}}},
  author = {Yu, Fisher and Koltun, Vladlen},
  date = {2016-04-30},
  url = {http://arxiv.org/abs/1511.07122},
  urldate = {2020-03-15},
  abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
  archivePrefix = {arXiv},
  eprint = {1511.07122},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/WBP67LDM/Yu and Koltun - 2016 - Multi-Scale Context Aggregation by Dilated Convolu.pdf},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{zeghidourWavesplit2020,
  title = {Wavesplit: {{End}}-to-{{End Speech Separation}} by {{Speaker Clustering}}},
  shorttitle = {Wavesplit},
  author = {Zeghidour, Neil and Grangier, David},
  date = {2020-02-20},
  url = {http://arxiv.org/abs/2002.08933},
  urldate = {2020-02-27},
  abstract = {We introduce Wavesplit, an end-to-end speech separation system. From a single recording of mixed speech, the model infers and clusters representations of each speaker and then estimates each source signal conditioned on the inferred representations. The model is trained on the raw waveform to jointly perform the two tasks. Our model infers a set of speaker representations through clustering, which addresses the fundamental permutation problem of speech separation. Moreover, the sequence-wide speaker representations provide a more robust separation of long, challenging sequences, compared to previous approaches. We show that Wavesplit outperforms the previous state-of-the-art on clean mixtures of 2 or 3 speakers (WSJ0-2mix, WSJ0-3mix), as well as in noisy (WHAM!) and reverberated (WHAMR!) conditions. As an additional contribution, we further improve our model by introducing online data augmentation for separation.},
  archivePrefix = {arXiv},
  eprint = {2002.08933},
  eprinttype = {arxiv},
  file = {/home/morris/docs/lib/Zotero/storage/GEDWV3FQ/Zeghidour and Grangier - 2020 - Wavesplit End-to-End Speech Separation by Speaker.pdf;/home/morris/docs/lib/Zotero/storage/RPIBXRN2/2002.html},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat},
  version = {1}
}


