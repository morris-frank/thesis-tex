\section{Related works}
In this chapter, we discuss previous research in supervised and semi-supervised source separation.

\subsection{ICA}


\subsection{Deep Latent-Variable Models}

For our process, we have observations from the data space \({\B{x}}\∈\mathcal{D}\) for which there exists an unknown data probability distribution \(p^*(\mathcal{D})\). We collect a data set \(\{\B{x}_1\…\B{x}_N\}\) with \(N\) samples. We introduce an approximate model with density\footnote{We write density and distribution interchangeably to denote a probability function.} \(p_{\B{\θ}}(\mathcal{D})\) and model parameters \(\B{\θ}\). Learning or modeling means finding the values for \(\B{\θ}\) which will give the closest approximation of the true underlying process:

\begin{equation}
    p_{\B{\θ}}(\mathcal{D}) \approx p^*(\mathcal{D})
\end{equation}

The model \(p_{\B{\θ}}\) has to be complex enough to be able to fit the data density while little enough parameters to be learned. Every choice for the form of the model will \I{induce} biases\footnote{called \I{inductive biases}} about what density we can model, even before we maximize a learning objective using the parameters \(\B{\θ}\).

In the following described models we assume the sampled data points \(\B{x}\) to be drawn from \(\mathcal{D}\) \I{independent and identically distributed}\footnote{meaning the sample of one datum does not depend on the other data points}. Therefore we can write the data log-likelihood as:

\begin{align}
    p_{\B{\θ}}(\mathcal{D})
    &= \Π_{\B{x}\∈\mathcal{D}} p_{\B{\θ}}(\B{x})\\
    \log p_{\B{\θ}}(\mathcal{D})
    &= \Σ_{\B{x}\∈\mathcal{D}} \log p_{\B{\θ}}(\B{x})
\end{align}

The maximum likelihood estimation of our model parameters maximizes this objective.

To form a latent-variable model we introduce a \I{latent variable}\footnote{Latent variables are part of the directed graphical model but not observed.}. The data likelihood now is the marginal density of the joint latent density:

\begin{equation}
    p_{\B{\θ}}(\B{x}) = \∫ p_{\B{\θ}}(\B{x},\B{z}) d\B{z}
\end{equation}

Typically we introduce a factorization of the joint. Most commonly and simplest:

\begin{equation}
    p_{\B{\θ}}(\B{x}) = \∫ p_{\B{\θ}}(\B{x}|\B{z})p(\B{z}) d\B{z}
    \label{eq:factorized_data_likelihood}
\end{equation}

\begin{marginfigure}%
    \input{figures/factorization_pgm}
    \caption{The graphical model with a introduced latent variable \(\B{z}\). Observed variables are shaded.}
    \label{fig:factorization_pgm}
\end{marginfigure}

This corresponds to the graphical model in which \(\B{z}\) is generative parent node of the observed \(\B{x}\), see~\cref{fig:factorization_pgm}. The density \(p(\B{z})\) is called the \I{prior distribution}.

If the latent is small, discrete, it might be possible to directly marginalize over it. If for example, \(\B{z}\) is a discrete random variable and the conditional \(p_{\B{\θ}}(\B{x}|\B{z})\) is a Gaussian distribution than the data model density \(p_{\B{\θ}}(\B{x})\) becomes a mixture-of-Gaussians, which we can directly estimate by maximum likelihood estimation of the data likelihood.

For more complicated models the data likelihood \(p_{\B{\θ}}(\B{x})\) as well as the model posterior \(p_{\B{\θ}}(\B{z}|\B{x})\) are intractable because of the integration over the latent \(\B{z}\) in \cref{eq:factorized_data_likelihood}.

To formalize the search for an intractable posterior into a tractable optimization problem we follow the \I{variational principle}~\autocite{jordanIntroduction1999} which introduces an approximate posterior distribution \(q_{\B{\φ}}(\B{z}|\B{x})\), also called the \I{inference model}. Again the choice of the model here carries inductive biases as such that even in asymptotic expectation we can not obtain the true posterior.

Following the derivation in~\autocite[p.~20]{kingmaIntroduction2019} we introduce the inference model into the data likelihood~\footnote{The first step is valid as \(q_{\B{\θ}}\) is a valid density function and thus integrates to one.}:

\begin{align}
    \log p_{\B{\θ}}(\B{x})
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})} \left[ \log p_{\B{\θ}}(\B{x}) \right]\\
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p_{\B{\θ}}(\B{x}, \B{z})}
          {p_{\B{\θ}}(\B{z}|\B{x})}
        \right]\\
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p_{\B{\θ}}(\B{x}, \B{z})}
          {q_{\B{\φ}}(\B{z}|\B{x})}
        \÷{q_{\B{\φ}}(\B{z}|\B{x})}
          {p_{\B{\θ}}(\B{z}|\B{x})}
        \right]\\
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p_{\B{\θ}}(\B{x}, \B{z})}
          {q_{\B{\φ}}(\B{z}|\B{x})}
        \right]
    +  \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{q_{\B{\φ}}(\B{z}|\B{x})}
          {p_{\B{\θ}}(\B{z}|\B{x})}
        \right]\\
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p_{\B{\θ}}(\B{x}, \B{z})}
            {q_{\B{\φ}}(\B{z}|\B{x})}
        \right]
    +  \KL[q_{\B{\φ}}(\B{z}|\B{x}) \|
           p_{\B{\θ}}(\B{z}|\B{x})  ]
\end{align}

Note that we separated the likelihood into two parts. The second part is the (positive) Kullback-Leibler divergence of the approximate posterior from the true intractable posterior. This unknown divergence states the `correctness' of our approximation~\footnote{More specifically the divergence marries two errors of our approximate model. First, it gives the error of our posterior estimation from the true posterior, by definition of divergence. Second, it specifies the error of our complete model likelihood from the marginal likelihood. This is called the \I{tightness} of the bound.}.

The first term is the \I{variational free energy}~\todo{explain free energy} or \I{evidence lower bound} (ELBO):

\begin{align}
    \elbo_{\B{\θ}, \B{\φ}}(\B{x})
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p_{\B{\θ}}(\B{x}, \B{z})}
          {q_{\B{\φ}}(\B{z}|\B{x})}
        \right]
    \label{eq:elbo}
\end{align}

We can introduce the same factorization as in~\cref{eq:factorized_data_likelihood}:

\begin{align}
    \elbo_{\B{\θ}, \B{\φ}}(\B{x})
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p_{\B{\θ}}(\B{x}|\B{z}) p(\B{z})}
          {q_{\B{\φ}}(\B{z}|\B{x})}
        \right]\\
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p(\B{z})}
          {q_{\B{\φ}}(\B{z}|\B{x})}
        \right]
    + \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log p_{\B{\θ}}(\B{x}|\B{z})\right]\\
    &= -\KL[q_{\B{\φ}}(\B{z}|\B{x})\|p(\B{z})]
    + \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log p_{\B{\θ}}(\B{x}|\B{z})\right]
    \label{eq:elbo}
\end{align}

Under this factorization, we separated the lower bound into two parts. First, the divergence of the approximate posterior from the latent prior distribution and second the data posterior likelihood from the latent~\footnote{this will later be the reconstruction error. How well can we return to the data density from latent space}.

The optimization of the \(\elbo_{\B{\θ}, \B{\φ}}\) allows us to jointly optimize the parameter sets \(\B{\θ}\) and \(\B{\φ}\). The gradient with respect to \(\B{\θ}\) can be estimated with an unbiased Monte Carlo estimate using data samples~\footnote{\( \∇_{\B{\θ}} \elbo_{\B{\θ}, \B{\φ}} \approxeq \∇_{\B{\θ}} \log p_{\B{\θ}} (\B{x}, \B{z}) \)}. We can \I{not} though do the same for the variational parameters \(\B{\φ}\), as the expectation of the ELBO is over the approximate posterior which depends on \(\B{\φ}\). By a change of variable of the latent variable we can make this gradient tractable, the so called \I{reparameterization trick}~\autocite{kingmaAutoEncoding2014}. We express the \(z\sim q_{\B{\θ}}\) as an random sample from a unparametrized source of entropy \(\B{\ε}\) and a parametrized transformation:

\begin{equation}
    \B{z} = f_{\B{\η}}(\B{\ε})
\end{equation}

For example for a Gaussian distribution we can express \(z\sim \N(\μ,\σ)\) as \(z = \μ + \σ\·\ε\) with \(\ε\sim \N(0,1)\) and \(\η = \{\μ,\σ\}\).

\subsection{The VAE framework}

VAE~\footnotemark[\value{footnote}]\autocite{rezendeStochastic2014}

The \β-VAE~\autocite{higginsBetaVAE2016} extends the VAE objective with an \(\β\) hyperparameter in front of the KL divergence. The value \(\β\) gives a constraint on the latent space controlling the capacity of it. Adapting \(\β\) gives a trade-off between the reconstruction quality of the autoencoder and the simplicity of the latent representations\footnotemark[\value{footnote}]. Using such a constraint is similar to the use of in the information bottleneck~\autocite{burgessUnderstanding2018}.

\subsection{Flow based models}

Another class of common deep latent models is based on \I{normalizing} flows~\autocite{tabakFamily2013}. A normalizing flow is a function \(f(\B{x})\) that maps the input density to a fixed, prescribed density \(p(\ε) = p(f(\B{x}))\), in that normalizing the density~\footnote{The extreme of this idea is, of course, an infinitesimal, continuous-time flow with a velocity field.}. They use a flow for the approximate posterior \(q_{\B{\φ}}(\B{z}|\B{x})\).  Again this is commonly set to be a factorized Gaussian distribution.

For a finite normalizing flow, we consider a chain of invertible, smooth mappings.

NICE~\autocite{dinhNICE2015}
- volume preserving transformations
- coupling layer
- triangular shape

Normalizing Flow~\autocite{rezendeVariational2016}


RealNVP~\autocite{dinhDensity2017} builds on top of NICE creating a more general, non-volume preserving, normalizing flow.

\begin{align}
    y_{1:d} &= x_{1:d}\\
    y_{d+1:D} &= x_{d+1:D} \odot \exp{(s(x_{1:d}))} + t(x_{1:d})
\end{align}

\begin{equation}
    \pf{y}{x^T} = \bM{\1_d & 0 \\ \pf{y_{d+1}:D}{x^T_{1:d}} & \diag{\left(\exp{(s(x_{1:d}))}\right)}}
\end{equation}

Glow~\autocite{kingmaGlow2018} extended the RealNVP by introducing invertible 1\×1-convolutions. Instead of having fixed masks and permutations for the computations of the affine parameters in the coupling layer, Glow learns a rotation matrix which mixes the channels. After mixing the input can always be split into the same two parts for the affine transformation. Further, the authors showed that training can be helped by initializing the last layer of each affine parameter network with zeros. This ensures that at the beginning without weight update each coupling layer behaves as an identity.

\subsection{Modelling raw audio}
Deep learning models as used for image applications are unsuitable for raw audio signals (\textit{time-domain}). Digital audio is sampled at high sample rates commonly 16kHz up to 44kHz. The features of interest lie at scales of strongly different magnitudes. Recognizing phase, frequency of a wave might require features at low ms intervals but modelling of speech or music features happens at the scale of seconds to minutes. As such a generative model for this domain has to model at these different scales.

The WaveNet~\autocite{vandenoordWaveNet2016} introduced an autoregressive generative model for raw audio. It is build upon the similar PixelCNN~\autocite[\protect\label{pixelcnn}]{vandenoordConditional2016} but adapted for the audio domain.  The WaveNet accomplishes this by using dilated causal convolutions a common tool in signal processing~\autocite{dutilleuxImplementation1990}. A dilated convolution uses a kernel with an inner stride. Using a stack of dilated convolutions increases the receptive field of the deep features without increasing the computational complexity, see~\cref{fig:wavenet}. Further, the convolutions are gated and the output is constructed from skip connections, refer to \cref{fig:wavenet_layer}. A gated feature, as known from the LSTM~\autocite{hochreiterLong1997a}, computes two outputs: one put through an sigmoid \(\σ(\·)\) activation and one through an \(\tanh(\·)\) activation. The idea being that the sigmoid (with an output range of \([0, 1]\)) regulates the amount of information, thereby gating it, while the \(\tanh\) (with a range of \([-1,1]\)) gives the magnitude of the feature. The output of the WaveNet is the sum of outflowing skip connections added after each (gated) hidden convolution. This helps fusing information from multiple time-scales (\I{low-level} to \I{high-level}). The original authors tested the model on multiple audio generation tasks. For this, they formulated the reconstruction objective as a multi-class recognition problem. Encoding the sound files with \μ-law encoding~\autocite{Recommendation1988}, discretizes the range \([-1, 1]\) to allow a set of \μ targets. Sound generation with a WaveNet is slow as the autoregressiveness requires the generation value by value\footnote{Why are they doing that then? The WaveNet setting is generating waveforms, by giving the previously generated values as the input and conditioning the process on target classes. Therefore the generation has to happen value-by-value.}. This can be alleviated by keeping intermediate hidden activations cached~\autocite{paineFast2016}. The WaveNet can be conditioned by adding the weighted conditionals in the gate and feature activations of the gated convolutions~\autocite{vandenoordConditional2016}.

\begin{marginfigure}
    \missingfigure[figwidth=2cm]{Show WaveNet hidden layer}%
    \label{fig:wavenet_layer}
\end{marginfigure}

\begin{figure}[]
    \input{figures/wavenet.tex}
    \caption{An example of how dilated convolutions are used in the WaveNet. We see three hidden layers with each a kernel size of two. By using the dilations the prediction of the new output element has a receptive field of 18. This convolution is \I{causal} as the prediction depends only on previous input values.}
    \label{fig:wavenet}
\end{figure}

\todo{explain PixelCNN++ even if I don't use it know?}

NSynth~\autocite{kalchbrennerEfficient2018}

In~\autocite{prengerWaveGlow2018}

FloWaveNet~\autocite{kimFloWaveNet2019a}

\subsection{Source separation}
WaveNet for Speech denoising\autocite{rethageWavenet2018}

WaveNet-VAE unsupervised speech rep learning\autocite{chorowskiUnsupervised2019}

\autocite{janssonSinging2017} were the first to use an U-Net architecture (see~\cref{fig:unet}) for source separation. They used a standard U-Net on spectrograms of musical data to extract the singing voice.

The Wave-U-Net\autocite{stollerWaveUNet2018} brings this idea into the time-domain. The downstreaming and upstreaming layers are replaced with WaveNet style layers.

\begin{marginfigure}
    \input{figures/unet}
    \caption{The U-Net}
    \label{fig:unet}
\end{marginfigure}

In~\autocite{lluisEndtoend2019} the authors adapted a time-domain WaveNet for musical source separation. The network is non-causal~\footnote{In this setting the WaveNet can be non-causal because the prediction happens from the given mix and is not autoregressive.} The WaveNet directly outputs the seaparated sources and is trained fully supervised. They show this simple setup performing well against spectrogram based baselines. Also, the experiments show a higher performance with a network that is deeper but less wide~\footnote{Deepness of a network refers to the number of hidden layers. Wideness refers to the number of kernels/features of each of these hidden layers. Making the WaveNet deeper significantly increases its receptive field.}

Demucs\autocite{defossezDemucs2019} is another extension building on the U-Net idea. Having a similar structure to the Wave-U-Net they introduce a bidirectional LSTM at the bottleneck. The LSTM is responsible for keeping long-term temporal informational by running over the high-level latent along the time dimension.
