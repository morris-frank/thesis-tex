\chapter{Research Question}%
\label{ch:question}

Source separation is the task of finding a set of latent sources \(\B{s} = {[\B{s}_1,\…\B{s}_k,\…,\B{s}_N]}^T \∈ \ℝ^{N\× T}\) to an observed mix of those sources \(\B{m}\∈\ℝ^{1\× T}\). The induced model proposes a mixing function \(\B{m} = f(\B{s})\). The task is to find an approximate inverse model \(g(\·)\) which retrieves \(\B{s}\):

\begin{align}
    \B{m} = f(\B{s})\\
    g(\B{m}) \approxeq \B{s}
\end{align}
In this learning setting \I{supervision} can happen in three ways: First the source signals are identified as being from class \(k\)~\footnote{For the setting of music think of the classes being \{\I{guitar},\I{piano},\I{voice},\…\}}. Second the tuples \((\B{m}, \B{s})\) are supervised giving us examples of mixes and their corresponding sources. And third knowing number \(k\) of sources. The first is not a supervision signal of the separation task and therefore

\begin{tcolorbox}
    \begin{enumerate}
        \item Can we learn a source separation model \(g(\·)\) by learning deep priors for the different source classes.
        \item Can we reduce this to an unsupervised setting. Unsupervised relating to the missing pairings of sources and mixes.
    \end{enumerate}
\end{tcolorbox}
