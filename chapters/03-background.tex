\chapter{Background}%
\label{ch:background}

In this section we introduce the minimally necessary background concepts our work is buidling upon. Our work marries two fields of research: on the one side generative, graphical models. With those one tries to model the data distribution of a observed variable by inducing dependencies of latent (unobserved) variables generating the observered variables. Modeling those dependencies, the meaningful latent variables allow us to build an intuitive generative story for the probabilistic model of the observed data. On the other hand we introduce the recent body of of work on how to practically model sound signals, in particular how this compares differently to modeling images.

First we introduce the concept of deep latent-variable models, which important classes of models exist and how to train them. Second we will shortly draw up the difficulties of modeling sound and which recent solutions exist and how they differ.

\section{Deep Latent-Variable Models}%
\label{sec:dlvm}
In this section we introduce the idea of latent variable and the idea of finding models of data using those latent variables. Further we discuss what practical ways are currently successfull ain estimating the model parameters and how they are trained.

For our process, we have observations from the data space \({\B{x}}\∈\mathcal{D}\) for which there exists an unknown data probability distribution \(p^*(\mathcal{D})\). We collect a data set \(\{\B{x}_1\…\B{x}_N\}\) with \(N\) samples. Further we introduce an approximate model with the density\footnote{We write density and distribution interchangeably to denote a probability function.} \(p_{\B{\θ}}(\mathcal{D})\) and model parameters \(\B{\θ}\). Learning or modeling means finding the values for \(\B{\θ}\) which will give the closest approximation of the true underlying process:

\begin{equation}
    p_{\B{\θ}}(\mathcal{D}) \approx p^*(\mathcal{D})
\end{equation}

The model \(p_{\B{\θ}}\) has to be complex enough to be able to fit the data density while little enough parameters to be learned. Every choice for the form of the model will \I{induce} biases\footnote{called \I{inductive biases}} about what density we can model, even before we maximize a learning objective using the parameters \(\B{\θ}\).

In the following described models we assume the sampled data points \(\B{x}\) to be drawn from \(\mathcal{D}\) \I{independent and identically distributed}\footnote{meaning the sample of one datum does not depend on the other data points and we all draw them the same way}. Therefore we can write the data log-likelihood as:

\begin{equation}
    \log p_{\B{\θ}}(\mathcal{D})
    = \Σ_{\B{x}\∈\mathcal{D}} \log p_{\B{\θ}}(\B{x})
\end{equation}

The maximum likelihood estimation of our model parameters maximizes this objective.

To form a latent-variable model we introduce a \I{latent variable}\footnote{Latent variables are part of the directed graphical model but not observed.}. This latent variable can be any random variable underlying the generation of a data sample in \(\mathcal{D}\). For example if we look at the generative story of a sound sample, a latent variable could be the sound source (the \I{instrument}). One can directly model the distribution of all musical notes played by all possible instruments. But we could also introduce the instrument as an underlying latent variable \(\B{z}\), which would then lead to one distribution over this categorical variable and separate conditional densities of the sounds of each instrument. One of them then modeling the distribution of sounds made by a harp, for example.

Now the data likelihood is the marginal density of the joint latent density. With one latent variable we get:

\begin{equation}
    p_{\B{\θ}}(\B{x}) = \∫ p_{\B{\θ}}(\B{x},\B{z}) d\B{z}
\end{equation}

Typically we introduce a factorization of the joint. The most common one and also the one from our previous example is:

\begin{equation}
    p_{\B{\θ}}(\B{x}) = \∫ p_{\B{\θ}}(\B{x}|\B{z})p(\B{z}) d\B{z}
    \label{eq:factorized_data_likelihood}
\end{equation}

\begin{marginfigure}%
    \input{figures/factorization_pgm}
    \caption{The graphical model with a introduced latent variable \(\B{z}\). Observed variables are shaded.}
    \label{fig:factorization_pgm}
\end{marginfigure}

This corresponds to the graphical model in which \(\B{z}\) is generative parent node of the observed \(\B{x}\), see~\cref{fig:factorization_pgm}. The density \(p(\B{z})\) is called the \I{prior distribution}.

If the latent is small, discrete, it might be possible to directly marginalize over it. If for example, \(\B{z}\) is a discrete random variable and the conditional \(p_{\B{\θ}}(\B{x}|\B{z})\) is a Gaussian distribution than the data model density \(p_{\B{\θ}}(\B{x})\) becomes a mixture-of-Gaussians, which we can directly estimate by maximum likelihood estimation of the data likelihood.

For more complicated models the data likelihood \(p_{\B{\θ}}(\B{x})\) as well as the model posterior \(p_{\B{\θ}}(\B{z}|\B{x})\) are intractable because of the integration over the latent \(\B{z}\) in \cref{eq:factorized_data_likelihood}.

To formalize the search for an intractable posterior into a tractable optimization problem we can follow the \I{variational principle}~\cite{jordanIntroduction1999} which introduces an approximate posterior distribution \(q_{\B{\φ}}(\B{z}|\B{x})\), also called the \I{inference model}. Again the choice of the model here carries inductive biases, as such that even in asymptotic expectation we can not obtain the true posterior.

Following the derivation in~\textcite[p.~20]{kingmaIntroduction2019} we introduce the inference model into the data likelihood~\footnote{The first step is valid as \(q_{\B{\θ}}\) is a valid density function and thus integrates to one.}:

\begin{align}
    \log p_{\B{\θ}}(\B{x})
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})} \left[ \log p_{\B{\θ}}(\B{x}) \right]\\
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p_{\B{\θ}}(\B{x}, \B{z})}
          {p_{\B{\θ}}(\B{z}|\B{x})}
        \right]\\
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p_{\B{\θ}}(\B{x}, \B{z})}
          {q_{\B{\φ}}(\B{z}|\B{x})}
        \÷{q_{\B{\φ}}(\B{z}|\B{x})}
          {p_{\B{\θ}}(\B{z}|\B{x})}
        \right]\\
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p_{\B{\θ}}(\B{x}, \B{z})}
          {q_{\B{\φ}}(\B{z}|\B{x})}
        \right]
    +  \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{q_{\B{\φ}}(\B{z}|\B{x})}
          {p_{\B{\θ}}(\B{z}|\B{x})}
        \right]\\
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p_{\B{\θ}}(\B{x}, \B{z})}
            {q_{\B{\φ}}(\B{z}|\B{x})}
        \right]
    +  \KL[q_{\B{\φ}}(\B{z}|\B{x}) \|
           p_{\B{\θ}}(\B{z}|\B{x})  ]
\end{align}

Note that we separated the likelihood into two parts. The second part is the (positive) Kullback-Leibler divergence of the approximate posterior from the true intractable posterior. This unknown divergence states the `correctness' of our approximation~\footnote{More specifically the divergence marries two errors of our approximate model. First, it gives the error of our posterior estimation from the true posterior, by definition of divergence. Second, it specifies the error of our complete model likelihood from the marginal likelihood. This is called the \I{tightness} of the bound.}.

The first term is the \I{variational free energy} or \I{evidence lower bound} (ELBO):

\begin{align}
    \elbo_{\B{\θ}, \B{\φ}}(\B{x})
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p_{\B{\θ}}(\B{x}, \B{z})}
          {q_{\B{\φ}}(\B{z}|\B{x})}
        \right]
    \label{eq:elbo}
\end{align}

We can introduce the same factorization as in~\cref{eq:factorized_data_likelihood}:

\begin{align}
    \elbo_{\B{\θ}, \B{\φ}}(\B{x})
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p_{\B{\θ}}(\B{x}|\B{z}) p(\B{z})}
          {q_{\B{\φ}}(\B{z}|\B{x})}
        \right]\\
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p(\B{z})}
          {q_{\B{\φ}}(\B{z}|\B{x})}
        \right]
    + \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log p_{\B{\θ}}(\B{x}|\B{z})\right]\\
    &= -\KL[q_{\B{\φ}}(\B{z}|\B{x})\|p(\B{z})]
    + \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log p_{\B{\θ}}(\B{x}|\B{z})\right]
    \label{eq:elbo}
\end{align}

Under this factorization, we separated the lower bound into two parts. First, the divergence of the approximate posterior from the latent prior distribution and second the data posterior likelihood from the latent~\footnote{This will later be the reconstruction error. How well can we return to the data density from latent space?}.

The optimization of the \(\elbo_{\B{\θ}, \B{\φ}}\) allows us to jointly optimize the parameter sets \(\B{\θ}\) and \(\B{\φ}\). The gradient with respect to \(\B{\θ}\) can be estimated with an unbiased Monte Carlo estimate using data samples~\footnote{\( \∇_{\B{\θ}} \elbo_{\B{\θ}, \B{\φ}} \approxeq \∇_{\B{\θ}} \log p_{\B{\θ}} (\B{x}, \B{z}) \)}. Though we can \I{not} do the same for the variational parameters \(\B{\φ}\), as the expectation of the ELBO is over the approximate posterior which depends on \(\B{\φ}\). By a change of variable of the latent variable we can make this gradient tractable, the so called \I{reparameterization trick}~\cite{kingmaAutoEncoding2014}. We express the \(z\sim q_{\B{\θ}}\) as an random sample from a unparametrized source of entropy \(\B{\ε}\) and a parametrized transformation:

\begin{equation}
    \B{z} = f_{\B{\η}}(\B{\ε})
\end{equation}

For example for a Gaussian distribution we can express \(z\sim \N(\μ,\σ)\) as \(z = \μ + \σ\·\ε\) with \(\ε\sim \N(0,1)\) and \(\η = \{\μ,\σ\}\).

\subsection{The VAE framework}

The variational autoencoder (VAE)~\footnotemark[\value{footnote}]~\cite{rezendeStochastic2014} is the previously introduced ideas together into a trainable model.

\itodo{more}

\begin{algorithm}
    \caption{Trainings procedure for a variational autoencoder}%
    \label{alg:vae}
    \begin{algorithmic}[1]
        \While{Training is not converged}
            \State \(\B{X} \sim \mathcal{D}\)\Comment{Sample random minibatch}
            \State \(\η \gets \text{EncoderNN}(\B{X})\)
            \State \(\B{\ε} \sim p(\B{\ε})\)
            \State \(\B{z} \gets f_{\B{\η}}(\B{\ε})\)
            \State \(\B{X}' \gets \text{DecoderNN}(\B{z})\)
        \EndWhile%
    \end{algorithmic}
\end{algorithm}

The \β-VAE~\cite{higginsBetaVAE2016} extends the VAE objective with an \(\β\) hyperparameter in front of the KL divergence. The value \(\β\) gives a constraint on the latent space, controlling the capacity of it. Adapting \(\β\) gives a trade-off between the reconstruction quality of the autoencoder and the simplicity of the latent representations\footnotemark[\value{footnote}]. Using such a constraint is similar to the use of the information bottleneck~\cite{burgessUnderstanding2018}.

\subsection{Flow based models}

Another class of common deep latent models is based on \I{normalizing} flows~\cite{tabakFamily2013}. A normalizing flow is a function \(f(\B{x})\) that maps the input density to a fixed, prescribed density \(p(\ε) = p(f(\B{x}))\), in that normalizing the density~\footnote{The extreme of this idea is, of course, an infinitesimal, continuous-time flow with a velocity field.}. They use a flow for the approximate posterior \(q_{\B{\φ}}(\B{z}|\B{x})\).  Again this is commonly set to be a factorized Gaussian distribution.

For a finite normalizing flow, we consider a chain of invertible, smooth mappings.

NICE~\cite{dinhNICE2015}
- volume preserving transformations
- coupling layer
- triangular shape

Normalizing Flow~\cite{rezendeVariational2016}


RealNVP~\cite{dinhDensity2017} builds on top of NICE creating a more general, non-volume preserving, normalizing flow.

\begin{align}
    y_{1:d} &= x_{1:d}\\
    y_{d+1:D} &= x_{d+1:D} \odot \exp{(s(x_{1:d}))} + t(x_{1:d})
\end{align}

\begin{equation}
    \pf{y}{x^T} = \bM{\1_d & 0 \\ \pf{y_{d+1}:D}{x^T_{1:d}} & \diag{\left(\exp{(s(x_{1:d}))}\right)}}
\end{equation}

Glow~\cite{kingmaGlow2018} extended the RealNVP by introducing invertible 1\×1-convolutions. Instead of having fixed masks and permutations for the computations of the affine parameters in the coupling layer, Glow learns a rotation matrix which mixes the channels. After mixing the input can always be split into the same two parts for the affine transformation. Further, the authors showed that training can be helped by initializing the last layer of each affine parameter network with zeros. This ensures that at the beginning without weight update each coupling layer behaves as an identity.

\section{Modeling audio}%
\label{sec:audio}
In the physical world a sound signal is a change of the density of the carrier air over time. As such it is a one-dimensional temporal signal\footnote{Sound waves in gases are purely compressive, therefore they cannot be polarized which would introduce a higher complexity. In solids a sound signal can have polarization, think of an earthquake with shear and pressure components.}. Humans sound preception is \I{stereo} as we have two ears and sense the changes in pressure at two different points in space simoustantly.

\subsection{Representations of audio}
The simplest form of digitally recording a sound signal is recording the air pressure at fixed intervals at one or multiple points in space with a microphone. Thie method is called Pulse-Code modulation (PCM). It introduces two parameters which will bias the result the record. First we have to pick a temporal frequency with which the pressure samples a taken, the so called sample rate. The Nyquist-Shannon theorem~\cite{kotelnikovCarrying1933} tells us that if the highest frequency in the true signal is \(B\ \si{\Hz}\) than with a sample rate of \(2\· B\ \si{\Hz}\) we will capture the complete signal with all frequencies. At a lower sample rate we might introduce aliasing effects. Second we have to represent each sample as a digital value. The pressure, or amplitude, at this time point is a physical value which is encoded. The microphone has a range of air pressure which w.l.o.g is set to \([-1, 1]\), where a value of \(0\) is no signal. The simplest form of encoding is Linear PCM where the amplitudes are quantized on a linear grid in the possible range. More advanced quantization include \(\μ\)-law encoding~\cite{Pulse1972} in which the quantization intervals are varied with the amplitude\footnote{The actual formula for calcuating the \(\μ\)-law quantization is: \[F(x) = \sgn{(x)} \· \÷{\ln{(1 + \μ |x|)}}{\ln{(1 + \μ)}}\]}. As human perception of loudness is logarithmic this encoding also quantizes the amplitude values on a logarithmic response curve. This makes it possible to achieve a higher signal-to-noise ratio at smaller encoding sizes for sound signals.


As an example, if encoding a sound signal with \(\SI{8}{\kHz}\) \(\SI{16}{\bit}\) Linear PCM we sample a amplitude value every \(\÷{1}{8000} \si{\s}\) and quantize those into \(2^{16} = 65536\) bins. A sample with 100 values is only \(\SI{12.5}{\ms}\) long!

A standard full piano with 88 keys tuned to \(\SI{440}{\Hz}\) at the \(\t{A}_4\) has a frequency range of \([\SI{27.5}{\Hz}, \SI{4186}{\Hz}]\) from the \(\t{A}_0\) to the \(\t{C}_8\), respectively. Therefore just for recording a standard piano we need a sampling rate of more than \(\SI{8}{\kHz}\). Beyond the piano many instruments exhibit even larger frequency content which leads to the conclusion that not just for recording but also for our goal of modeling musical sounds a resonably high sampling rate is crucial.

\itodo{Time-domain vs spectrogram, what is a spectrogram, missing phase information, complex spectrograms…}

\subsection{Modeling raw audio}
Deep learning models as used for image applications are unsuitable for raw audio signals (signals in \I{time-domain}). Digital audio is sampled at high sample rates, commonly 16kHz up to 44kHz. The features of interest lie at scales of strongly different magnitudes. Recognizing the local-structure of a signal, like frequency and timbre, might require features at short intervals (\(\approx\) tens of milliseconds) but modeling of speech or music features happens at the scale of seconds to minutes. As such a generative model for this domain has to model at these different scales.

\begin{figure}[]
    \input{figures/wavenet.tex}
    \caption{An example of how dilated convolutions are used in the WaveNet. We see three hidden layers with each a kernel size of two. By using the dilations the prediction of the new output element has a receptive field of 18. This convolution is \I{causal} as the prediction depends only on previous input values. Causality is enforced through asymmetric padding.}
    \label{fig:wavenet}
\end{figure}

The \B{WaveNet}~\cite{vandenoordWaveNet2016} introduced an autoregressive generative model for raw audio. It is build upon the similar PixelCNN~\cite[\protect\label{pixelcnn}]{vandenoordConditional2016} but adapted for the audio domain.  The WaveNet accomplishes this by using dilated causal convolutions~\footnote{The \I{à trous} alogrithm (\textcite{holschneiderRealTime1990}), a common tool in signal processing, uses a Wavelet kernel that is dilated to multiple scales. A dilated convolution, first used in \textcite{yuMultiScale2016} differs in that the convolution operator it-self is \I{dilated}, having an internal stride.}.  Using a stack of dilated convolutions increases the receptive field of the deep features without increasing the computational complexity, see~\cref{fig:wavenet}. Further, the convolutions are gated and the output is constructed from skip connections. For the sturcture of a hidden layer refer to \cref{fig:wavenet_layer}. A gated feature, as known from the LSTM~\cite{hochreiterLong1997a}, computes two outputs: one put through an sigmoid \(\σ(\·)\) activation and one through an \(\tanh(\·)\) activation. The idea being that the sigmoid (with an output range of \([0, 1]\)) regulates the amount of information, thereby gating it, while the \(\tanh\) (with a range of \([-1,1]\)) gives the magnitude of the feature. The output of the WaveNet is the sum of outflowing skip connections added after each (gated) hidden convolution. This helps fusing information from multiple time-scales (\I{low-level} to \I{high-level}) and makes training easier~\cite{szegedyGoing2015}. The original authors tested the model on multiple audio generation tasks. They used a \μ-law encoding~\cite{Recommendation1988} which discretizes the range \([-1, 1]\) to allow a set of \μ targets and an multi-class cross-entropy training objective. While being quite unnatural this is done to avoid making any assumptions about the target distribution. Sound generation with a WaveNet is slow as the autoregressiveness requires the generation value by value\footnote{Why are they doing that then? The WaveNet setting is generating waveforms, by giving the previously generated values as the input and conditioning the process on target classes, \(p(x_t|x_{1:t},c_t)\). Therefore the generation has to happen value-by-value.}. This can be alleviated by keeping intermediate hidden activations cached~\cite{paineFast2016}. The WaveNet can be conditioned by adding the weighted conditionals in the gate and feature activations of the gated convolutions~\cite{vandenoordConditional2016}.

\begin{marginfigure}
    \input{figures/wavenet_layer.tex}
    \caption{Hidden layer as in the WaveNet. Infomration flows from left, gets dilated and through the gate and filter. The result gets added to the skip flow and the hidden feature, each with a channel mixer before.}%
    \label{fig:wavenet_layer}
\end{marginfigure}

\todo{add PixelCNN++?}

The first generative sounds model to emply a WaveNet is \B{NSynth}~\cite{kalchbrennerEfficient2018}. They construct a VAE where encoder and decoder are both WaveNet-like. The so-called \I{non-causal temporal encoder} uses a stack of dilated residual non-causal convolutions. The convolutions are not gated and no skip-connections are used. The decoder is a WaveNet taking the original input chunk as an input and predicits the next value of the sound sample, while being conditioned on the latent variable. The authors use this model to learn latent temporal codes from a new large set of notes played by natural and synthesized instruments. The latent of the VAE is conditioned on the pitch of these notes. While the model is difficult to train, they show great improvement of the WaveNet-based VAE compared to a spectral-based VAE.

\textcite{chorowskiUnsupervised2019} presents another WaveNet-based VAE. They are learning speech representations, unsupervised. The encoder is a residual convnet and takes the MFCC of the signal as its input. As the bottleneck they found a VQ codebook~\cite{vandenoordNeural2017} to be most successfull. The decoder is an autoregressive WaveNet conditioned on the latent features. Note again that this model infers the latent features from mel-cepstra but reconstructs the signal with the WaveNet in time-domain.

In~\cite{prengerWaveGlow2018}

FloWaveNet~\cite{kimFloWaveNet2019a}
