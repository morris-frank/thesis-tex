\chapter{Background}%
\label{ch:background}

In this section we
In general this research work is build on prior work first on deep generative models, or latetn models, which try to estimate the distribution / density of a data space to find smooth manifolds. Second we will give an overview of contemporary works on using deep models for music and speech analysis and generation.

\subsection{Deep Latent-Variable Models}

In this section we introduce the idea of latent variable and the idea of finding models of data using those laten variables. Further we discuss what practical ways are currently successfull ain estimating the model parameters and how they are trained.

For our process, we have observations from the data space \({\B{x}}\∈\mathcal{D}\) for which there exists an unknown data probability distribution \(p^*(\mathcal{D})\). We collect a data set \(\{\B{x}_1\…\B{x}_N\}\) with \(N\) samples. We introduce an approximate model with density\footnote{We write density and distribution interchangeably to denote a probability function.} \(p_{\B{\θ}}(\mathcal{D})\) and model parameters \(\B{\θ}\). Learning or modeling means finding the values for \(\B{\θ}\) which will give the closest approximation of the true underlying process:

\begin{equation}
    p_{\B{\θ}}(\mathcal{D}) \approx p^*(\mathcal{D})
\end{equation}

The model \(p_{\B{\θ}}\) has to be complex enough to be able to fit the data density while little enough parameters to be learned. Every choice for the form of the model will \I{induce} biases\footnote{called \I{inductive biases}} about what density we can model, even before we maximize a learning objective using the parameters \(\B{\θ}\).

In the following described models we assume the sampled data points \(\B{x}\) to be drawn from \(\mathcal{D}\) \I{independent and identically distributed}\footnote{meaning the sample of one datum does not depend on the other data points}. Therefore we can write the data log-likelihood as:

\begin{equation}
    \log p_{\B{\θ}}(\mathcal{D})
    = \Σ_{\B{x}\∈\mathcal{D}} \log p_{\B{\θ}}(\B{x})
\end{equation}

The maximum likelihood estimation of our model parameters maximizes this objective.

To form a latent-variable model we introduce a \I{latent variable}\footnote{Latent variables are part of the directed graphical model but not observed.}. The data likelihood now is the marginal density of the joint latent density:

\begin{equation}
    p_{\B{\θ}}(\B{x}) = \∫ p_{\B{\θ}}(\B{x},\B{z}) d\B{z}
\end{equation}

Typically we introduce a factorization of the joint. Most commonly and simplest:

\begin{equation}
    p_{\B{\θ}}(\B{x}) = \∫ p_{\B{\θ}}(\B{x}|\B{z})p(\B{z}) d\B{z}
    \label{eq:factorized_data_likelihood}
\end{equation}

\begin{marginfigure}%
    \input{figures/factorization_pgm}
    \caption{The graphical model with a introduced latent variable \(\B{z}\). Observed variables are shaded.}
    \label{fig:factorization_pgm}
\end{marginfigure}

This corresponds to the graphical model in which \(\B{z}\) is generative parent node of the observed \(\B{x}\), see~\cref{fig:factorization_pgm}. The density \(p(\B{z})\) is called the \I{prior distribution}.

If the latent is small, discrete, it might be possible to directly marginalize over it. If for example, \(\B{z}\) is a discrete random variable and the conditional \(p_{\B{\θ}}(\B{x}|\B{z})\) is a Gaussian distribution than the data model density \(p_{\B{\θ}}(\B{x})\) becomes a mixture-of-Gaussians, which we can directly estimate by maximum likelihood estimation of the data likelihood.

For more complicated models the data likelihood \(p_{\B{\θ}}(\B{x})\) as well as the model posterior \(p_{\B{\θ}}(\B{z}|\B{x})\) are intractable because of the integration over the latent \(\B{z}\) in \cref{eq:factorized_data_likelihood}.

To formalize the search for an intractable posterior into a tractable optimization problem we follow the \I{variational principle}~\cite{jordanIntroduction1999} which introduces an approximate posterior distribution \(q_{\B{\φ}}(\B{z}|\B{x})\), also called the \I{inference model}. Again the choice of the model here carries inductive biases as such that even in asymptotic expectation we can not obtain the true posterior.

Following the derivation in~\textcite[p.~20]{kingmaIntroduction2019} we introduce the inference model into the data likelihood~\footnote{The first step is valid as \(q_{\B{\θ}}\) is a valid density function and thus integrates to one.}:

test~\cite{jordanIntroduction1999}

\begin{align}
    \log p_{\B{\θ}}(\B{x})
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})} \left[ \log p_{\B{\θ}}(\B{x}) \right]\\
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p_{\B{\θ}}(\B{x}, \B{z})}
          {p_{\B{\θ}}(\B{z}|\B{x})}
        \right]\\
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p_{\B{\θ}}(\B{x}, \B{z})}
          {q_{\B{\φ}}(\B{z}|\B{x})}
        \÷{q_{\B{\φ}}(\B{z}|\B{x})}
          {p_{\B{\θ}}(\B{z}|\B{x})}
        \right]\\
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p_{\B{\θ}}(\B{x}, \B{z})}
          {q_{\B{\φ}}(\B{z}|\B{x})}
        \right]
    +  \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{q_{\B{\φ}}(\B{z}|\B{x})}
          {p_{\B{\θ}}(\B{z}|\B{x})}
        \right]\\
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p_{\B{\θ}}(\B{x}, \B{z})}
            {q_{\B{\φ}}(\B{z}|\B{x})}
        \right]
    +  \KL[q_{\B{\φ}}(\B{z}|\B{x}) \|
           p_{\B{\θ}}(\B{z}|\B{x})  ]
\end{align}

Note that we separated the likelihood into two parts. The second part is the (positive) Kullback-Leibler divergence of the approximate posterior from the true intractable posterior. This unknown divergence states the `correctness' of our approximation~\footnote{More specifically the divergence marries two errors of our approximate model. First, it gives the error of our posterior estimation from the true posterior, by definition of divergence. Second, it specifies the error of our complete model likelihood from the marginal likelihood. This is called the \I{tightness} of the bound.}.

The first term is the \I{variational free energy}~\todo{explain free energy} or \I{evidence lower bound} (ELBO):

\begin{align}
    \elbo_{\B{\θ}, \B{\φ}}(\B{x})
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p_{\B{\θ}}(\B{x}, \B{z})}
          {q_{\B{\φ}}(\B{z}|\B{x})}
        \right]
    \label{eq:elbo}
\end{align}

We can introduce the same factorization as in~\cref{eq:factorized_data_likelihood}:

\begin{align}
    \elbo_{\B{\θ}, \B{\φ}}(\B{x})
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p_{\B{\θ}}(\B{x}|\B{z}) p(\B{z})}
          {q_{\B{\φ}}(\B{z}|\B{x})}
        \right]\\
    &= \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log
        \÷{p(\B{z})}
          {q_{\B{\φ}}(\B{z}|\B{x})}
        \right]
    + \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log p_{\B{\θ}}(\B{x}|\B{z})\right]\\
    &= -\KL[q_{\B{\φ}}(\B{z}|\B{x})\|p(\B{z})]
    + \E_{q_{\B{\θ}}(\B{z}|\B{x})}
        \left[\log p_{\B{\θ}}(\B{x}|\B{z})\right]
    \label{eq:elbo}
\end{align}

Under this factorization, we separated the lower bound into two parts. First, the divergence of the approximate posterior from the latent prior distribution and second the data posterior likelihood from the latent~\footnote{this will later be the reconstruction error. How well can we return to the data density from latent space}.

The optimization of the \(\elbo_{\B{\θ}, \B{\φ}}\) allows us to jointly optimize the parameter sets \(\B{\θ}\) and \(\B{\φ}\). The gradient with respect to \(\B{\θ}\) can be estimated with an unbiased Monte Carlo estimate using data samples~\footnote{\( \∇_{\B{\θ}} \elbo_{\B{\θ}, \B{\φ}} \approxeq \∇_{\B{\θ}} \log p_{\B{\θ}} (\B{x}, \B{z}) \)}. We can \I{not} though do the same for the variational parameters \(\B{\φ}\), as the expectation of the ELBO is over the approximate posterior which depends on \(\B{\φ}\). By a change of variable of the latent variable we can make this gradient tractable, the so called \I{reparameterization trick}~\cite{kingmaAutoEncoding2014}. We express the \(z\sim q_{\B{\θ}}\) as an random sample from a unparametrized source of entropy \(\B{\ε}\) and a parametrized transformation:

\begin{equation}
    \B{z} = f_{\B{\η}}(\B{\ε})
\end{equation}

For example for a Gaussian distribution we can express \(z\sim \N(\μ,\σ)\) as \(z = \μ + \σ\·\ε\) with \(\ε\sim \N(0,1)\) and \(\η = \{\μ,\σ\}\).

\subsection{The VAE framework}

VAE~\footnotemark[\value{footnote}]\cite{rezendeStochastic2014}

The \β-VAE~\cite{higginsBetaVAE2016} extends the VAE objective with an \(\β\) hyperparameter in front of the KL divergence. The value \(\β\) gives a constraint on the latent space controlling the capacity of it. Adapting \(\β\) gives a trade-off between the reconstruction quality of the autoencoder and the simplicity of the latent representations\footnotemark[\value{footnote}]. Using such a constraint is similar to the use of in the information bottleneck~\cite{burgessUnderstanding2018}.

\subsection{Flow based models}

Another class of common deep latent models is based on \I{normalizing} flows~\cite{tabakFamily2013}. A normalizing flow is a function \(f(\B{x})\) that maps the input density to a fixed, prescribed density \(p(\ε) = p(f(\B{x}))\), in that normalizing the density~\footnote{The extreme of this idea is, of course, an infinitesimal, continuous-time flow with a velocity field.}. They use a flow for the approximate posterior \(q_{\B{\φ}}(\B{z}|\B{x})\).  Again this is commonly set to be a factorized Gaussian distribution.

For a finite normalizing flow, we consider a chain of invertible, smooth mappings.

NICE~\cite{dinhNICE2015}
- volume preserving transformations
- coupling layer
- triangular shape

Normalizing Flow~\cite{rezendeVariational2016}


RealNVP~\cite{dinhDensity2017} builds on top of NICE creating a more general, non-volume preserving, normalizing flow.

\begin{align}
    y_{1:d} &= x_{1:d}\\
    y_{d+1:D} &= x_{d+1:D} \odot \exp{(s(x_{1:d}))} + t(x_{1:d})
\end{align}

\begin{equation}
    \pf{y}{x^T} = \bM{\1_d & 0 \\ \pf{y_{d+1}:D}{x^T_{1:d}} & \diag{\left(\exp{(s(x_{1:d}))}\right)}}
\end{equation}

Glow~\cite{kingmaGlow2018} extended the RealNVP by introducing invertible 1\×1-convolutions. Instead of having fixed masks and permutations for the computations of the affine parameters in the coupling layer, Glow learns a rotation matrix which mixes the channels. After mixing the input can always be split into the same two parts for the affine transformation. Further, the authors showed that training can be helped by initializing the last layer of each affine parameter network with zeros. This ensures that at the beginning without weight update each coupling layer behaves as an identity.

\subsection{Modeling raw audio}
Deep learning models as used for image applications are unsuitable for raw audio signals (signals in \I{time-domain}). Digital audio is sampled at high sample rates commonly 16kHz up to 44kHz. The features of interest lie at scales of strongly different magnitudes. Recognizing the local-structure of a signal, like frequency and timbre, might require features at short intervals (\(\approx\) tens of milliseconds) but modeling of speech or music features happens at the scale of seconds to minutes. As such a generative model for this domain has to model at these different scales.

\begin{figure}[]
    \input{figures/wavenet.tex}
    \caption{An example of how dilated convolutions are used in the WaveNet. We see three hidden layers with each a kernel size of two. By using the dilations the prediction of the new output element has a receptive field of 18. This convolution is \I{causal} as the prediction depends only on previous input values. Causality is enforced through asymmetric padding.}
    \label{fig:wavenet}
\end{figure}

The \B{WaveNet}~\cite{vandenoordWaveNet2016} introduced an autoregressive generative model for raw audio. It is build upon the similar PixelCNN~\cite[\protect\label{pixelcnn}]{vandenoordConditional2016} but adapted for the audio domain.  The WaveNet accomplishes this by using dilated causal convolutions~\footnote{The \I{à trous} alogrithm (\textcite{holschneiderRealTime1990}), a common tool in signal processing, uses a Wavelet kernel that is dilated to multiple scales. A dilated convolution, first used in \textcite{yuMultiScale2016} differs in that the convolution operator it-self is \I{dilated}, having an internal stride.}.  Using a stack of dilated convolutions increases the receptive field of the deep features without increasing the computational complexity, see~\cref{fig:wavenet}. Further, the convolutions are gated and the output is constructed from skip connections. For the sturcture of a hidden layer refer to \cref{fig:wavenet_layer}. A gated feature, as known from the LSTM~\cite{hochreiterLong1997a}, computes two outputs: one put through an sigmoid \(\σ(\·)\) activation and one through an \(\tanh(\·)\) activation. The idea being that the sigmoid (with an output range of \([0, 1]\)) regulates the amount of information, thereby gating it, while the \(\tanh\) (with a range of \([-1,1]\)) gives the magnitude of the feature. The output of the WaveNet is the sum of outflowing skip connections added after each (gated) hidden convolution. This helps fusing information from multiple time-scales (\I{low-level} to \I{high-level}) and makes training easier~\cite{szegedyGoing2015}. The original authors tested the model on multiple audio generation tasks. They used a \μ-law encoding~\cite{Recommendation1988} which discretizes the range \([-1, 1]\) to allow a set of \μ targets and an multi-class cross-entropy training objective. While being quite unnatural this is done to avoid making any assumptions about the target distribution. Sound generation with a WaveNet is slow as the autoregressiveness requires the generation value by value\footnote{Why are they doing that then? The WaveNet setting is generating waveforms, by giving the previously generated values as the input and conditioning the process on target classes, \(p(x_t|x_{1:t},c_t)\). Therefore the generation has to happen value-by-value.}. This can be alleviated by keeping intermediate hidden activations cached~\cite{paineFast2016}. The WaveNet can be conditioned by adding the weighted conditionals in the gate and feature activations of the gated convolutions (see~\textcite{vandenoordConditional2016}).

\begin{marginfigure}
    \input{figures/wavenet_layer.tex}
    \caption{Hidden layer as in the WaveNet. Infomration flows from left, gets dilated and through the gate and filter. The result gets added to the skip flow and the hidden feature, each with a channel mixer before.}%
    \label{fig:wavenet_layer}
\end{marginfigure}

% INTERANL TODO: explain PixelCNN++ even if I don't use it know? %

The first generative sounds model to emply a WaveNet is \B{NSynth}~\cite{kalchbrennerEfficient2018}. They construct a VAE where encoder and decoder are both WaveNet-like. The, so-called, \I{non-causal temporal encoder} uses a stack of dilated residual non-causal convolutions. The convolutions are not gated and no skip-connections are used. The decoder is a WaveNet taking the original input chunk as an input and predicits the next value of the sound sample, while being conditioned on the latent variable. The authors use this model to learn latent temporal codes from a new large set of notes played by natural and synthesized instruments. The latent of the VAE is conditioned on the pitch of these notes. While the model is difficult to train, they show great improvement of the WaveNet-based VAE compared to a spectral-based VAE.

\textcite{chorowskiUnsupervised2019} presents another WaveNet-based VAE. They are learning speech representations, unsupervised. The encoder is a residual convnet and takes the MFCC of the signal as its input. As the bottleneck they found a VQ codebook~\cite{vandenoordNeural2017} to be most successfull. The decoder is an autoregressive WaveNet conditioned on the latent features. Note again that this model infers the latent features from mel-cepstra but reconstructs the signal with the WaveNet in time-domain.

In~\cite{prengerWaveGlow2018}

FloWaveNet~\cite{kimFloWaveNet2019a}
